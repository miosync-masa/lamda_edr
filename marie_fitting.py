"""
=============================================================================
Marie Antoinette Enhanced Fitting System
å¤šæ§˜ä½“ãƒ™ãƒ¼ã‚¹ã®ç ´æ–­åˆ¤å®šãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 

å®Ÿéš›ã«ä½¿ç”¨ã—ã¦ã„ã‚‹æœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼š
- Phase 0: ç‰©ç†åˆ¶ç´„ã®ã¿ã®æ•™å¸«ãªã—å­¦ç¿’
- å®‰å…¨/å±é™ºå¤šæ§˜ä½“ã®æ§‹ç¯‰
- å‹•çš„é–¾å€¤ã«ã‚ˆã‚‹åˆ¤å®š
- ç›¸å¯¾è·é›¢ãƒ™ãƒ¼ã‚¹ã®é«˜ç²¾åº¦åˆ¤å®š

Author: é£¯æ³‰çœŸé“ + ç’°
Date: 2025-01-19
=============================================================================
"""

import jax
import jax.numpy as jnp
from jax import jit, vmap, grad
import numpy as np
import optax
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass

# edr_coreã‹ã‚‰å¿…è¦ãªæ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from edr_core import (
    MaterialParams,
    EDRParams,
    PressSchedule,
    ExpBinary,
    FLCPoint,
    schedule_to_jax_dict,
    mat_to_jax_dict,
    transform_params_jax,
    init_edr_params_jax,
    simulate_lambda_jax,
    triax_from_path_jax,
    smooth_signal_jax,
)

# manifold_libã‹ã‚‰å¤šæ§˜ä½“æ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from manifold_lib import (
    SafeManifold,
    DangerManifold,
    SafeManifoldBuilder,
    DangerManifoldBuilder,
    ManifoldAnalyzer,
    compute_gram,
    batch_gram_distance,
    RegularizationTerms,
)

# =============================================================================
# Section 1: æå¤±é–¢æ•°ç¾¤ï¼ˆedr_fit.pyã‹ã‚‰ç§»æ¤ãƒ»æ”¹è‰¯ï¼‰
# =============================================================================

@jit
def loss_single_exp_jax(schedule_dict, mat_dict, edr_dict, failed):
    """å˜ä¸€å®Ÿé¨“ã®æå¤±ï¼ˆåŸºæœ¬ç‰ˆï¼‰"""
    res = simulate_lambda_jax(schedule_dict, mat_dict, edr_dict)
    
    Lambda_smooth = smooth_signal_jax(res["Lambda"], window_size=11)
    
    peak = jnp.max(Lambda_smooth)
    D_end = res["Damage"][-1]
    
    margin = 0.08
    Dcrit = 0.01
    delta = 0.03
    
    # failed == 1ã®å ´åˆï¼ˆç ´æ–­ï¼‰
    condition_met = (peak > edr_dict['Lambda_crit']) & (D_end > Dcrit)
    
    peak_penalty = jnp.maximum(0.0, edr_dict['Lambda_crit'] - peak)
    D_penalty = jnp.maximum(0.0, Dcrit - D_end)
    loss_fail_not_met = 10.0 * (peak_penalty**2 + D_penalty**2)
    
    margin_loss = jnp.where(
        peak < edr_dict['Lambda_crit'] + margin,
        (edr_dict['Lambda_crit'] + margin - peak)**2,
        0.0
    )
    D_loss = jnp.where(
        D_end < 2*Dcrit,
        (2*Dcrit - D_end)**2,
        0.0
    )
    loss_fail_met = margin_loss + D_loss
    
    loss_fail = jnp.where(condition_met, loss_fail_met, loss_fail_not_met)
    
    # failed == 0ã®å ´åˆï¼ˆå®‰å…¨ï¼‰
    loss_safe_peak = jnp.where(
        peak > edr_dict['Lambda_crit'] - delta,
        (peak - (edr_dict['Lambda_crit'] - delta))**2 * 3.0,
        0.0
    )
    loss_safe_D = jnp.where(
        D_end >= 0.5*Dcrit,
        10.0 * (D_end - 0.5*Dcrit)**2,
        0.0
    )
    loss_safe = loss_safe_peak + loss_safe_D
    
    return jnp.where(failed == 1, loss_fail, loss_safe)

def loss_fn_jax(raw_params, exps, mat):
    """ãƒãƒƒãƒæå¤±é–¢æ•°ï¼ˆåŸºæœ¬ç‰ˆï¼‰"""
    edr_dict = transform_params_jax(raw_params)
    mat_dict = mat_to_jax_dict(mat)
    
    total_loss = 0.0
    for exp in exps:
        schedule_dict = schedule_to_jax_dict(exp.schedule)
        loss = loss_single_exp_jax(schedule_dict, mat_dict, edr_dict, exp.failed)
        total_loss += loss
    
    return total_loss / len(exps)

def predict_flc_from_sim_jax(beta, mat_dict, edr_dict, major_rate=0.6, duration=1.0):
    """Î›(t)ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰FLCé™ç•Œç‚¹ã‚’æŠ½å‡º"""
    dt = 1e-3
    N = int(duration/dt) + 1
    t = jnp.linspace(0, duration, N)
    epsM = major_rate * t
    epsm = beta * epsM
    
    schedule_dict = {
        't': t,
        'eps_maj': epsM,
        'eps_min': epsm,
        'triax': jnp.full(N, triax_from_path_jax(beta)),
        'mu': jnp.full(N, 0.08),
        'pN': jnp.full(N, 200e6),
        'vslip': jnp.full(N, 0.02),
        'htc': jnp.full(N, 8000.0),
        'Tdie': jnp.full(N, 293.15),
        'contact': jnp.full(N, 1.0),
        'T0': 293.15
    }
    
    res = simulate_lambda_jax(schedule_dict, mat_dict, edr_dict)
    Lambda_raw = res["Lambda"]
    Lambda_smooth = smooth_signal_jax(Lambda_raw, window_size=11)
    
    epsM_trimmed = epsM[:-1]
    
    # å¾®åˆ†å¯èƒ½ãªé™ç•Œç‚¹æ¤œå‡º
    exceed = jnp.maximum(Lambda_smooth - edr_dict['Lambda_crit'], 0.0)
    w = jnp.exp(jnp.minimum(10.0 * exceed, 10.0))
    w = w / (jnp.sum(w) + 1e-12)
    Em = jnp.sum(w * epsM_trimmed)
    Em = jnp.where(jnp.isnan(Em), epsM_trimmed[-1], Em)
    em = beta * Em
    
    return Em, em, Lambda_smooth

def loss_flc_true_jax(raw_params, flc_pts_data, mat_dict):
    """FLCæå¤±é–¢æ•°"""
    edr_dict = transform_params_jax(raw_params)
    
    flc_err = 0.0
    for i in range(len(flc_pts_data['path_ratios'])):
        beta = flc_pts_data['path_ratios'][i]
        Em_gt = flc_pts_data['major_limits'][i]
        em_gt = flc_pts_data['minor_limits'][i]
        
        Em_pred, em_pred, _ = predict_flc_from_sim_jax(beta, mat_dict, edr_dict)
        
        # Î²ä¾å­˜ã®é‡ã¿ä»˜ã‘
        w = jnp.where(jnp.abs(beta - 0.5) < 0.1, 6.0,
                      jnp.where(jnp.abs(beta) < 0.1, 1.8, 1.0))
        
        flc_err += w * ((Em_pred - Em_gt)**2 + (em_pred - em_gt)**2)
    
    flc_err = flc_err / len(flc_pts_data['path_ratios'])
    
    # Vå­—å½¢çŠ¶ã¨æ»‘ã‚‰ã‹ã•ã®æ­£å‰‡åŒ–
    beta_batch = jnp.linspace(-0.6, 0.6, 21)
    Em_curve = []
    for beta in beta_batch:
        Em, _, _ = predict_flc_from_sim_jax(beta, mat_dict, edr_dict)
        Em_curve.append(Em)
    Em_curve = jnp.array(Em_curve)
    
    center_idx = len(beta_batch) // 2
    center = Em_curve[center_idx]
    valley_loss = 0.1 * jnp.mean(jnp.maximum(0.0, Em_curve - center))
    
    grad2 = jnp.diff(jnp.diff(Em_curve))
    smoothness_loss = 0.05 * jnp.mean(grad2**2) + 0.02 * jnp.mean(jnp.abs(grad2))
    
    valley_weight = jnp.clip(jnp.var(Em_curve), 0.05, 0.3)
    valley_loss = valley_weight * valley_loss
    
    return flc_err + valley_loss + smoothness_loss

# =============================================================================
# Section 2: å‹•çš„é–¾å€¤è¨ˆç®—ã¨åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ 
# =============================================================================

@dataclass
class ThresholdConfig:
    """é–¾å€¤è¨­å®š"""
    safe_upper: float      # å®‰å…¨å´ä¸Šé™
    danger_lower: float    # å±é™ºå´ä¸‹é™
    optimal: float         # æœ€é©åˆ†é›¢ç‚¹
    method: str           # è¨ˆç®—æ–¹æ³•

def calculate_dynamic_thresholds(
    exps: List[ExpBinary],
    mat_dict: Dict,
    edr_dict: Dict,
    safe_manifold: SafeManifold,
    weights: Optional[Dict[str, float]] = None,
    verbose: bool = True
) -> ThresholdConfig:
    """
    å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‹•çš„ã«é–¾å€¤ã‚’è¨ˆç®—
    """
    if weights is None:
        weights = {
            'tv': 0.1,
            'jump': 0.5,
            'topo': 0.1,
            'l1': 1e-3
        }
    
    analyzer = ManifoldAnalyzer(weights)
    
    safe_scores = []
    danger_scores = []
    
    for exp in exps:
        schedule_dict = schedule_to_jax_dict(exp.schedule)
        res = simulate_lambda_jax(schedule_dict, mat_dict, edr_dict)
        Lambda = res["Lambda"]
        
        score = float(analyzer.compute_safety_score(Lambda, safe_manifold))
        
        if exp.failed == 0:
            safe_scores.append(score)
        else:
            danger_scores.append(score)
    
    # çµ±è¨ˆçš„ã«æ±ºå®š
    safe_mean = np.mean(safe_scores)
    safe_std = np.std(safe_scores)
    danger_mean = np.mean(danger_scores)
    danger_std = np.std(danger_scores)
    
    # 2Ïƒãƒ«ãƒ¼ãƒ«ã§é–¾å€¤è¨­å®š
    safe_upper = safe_mean + 2 * safe_std
    danger_lower = danger_mean - 2 * danger_std
    
    # Fisheråˆ¤åˆ¥ã«ã‚ˆã‚‹æœ€é©åˆ†é›¢ç‚¹
    if safe_std + danger_std > 0:
        optimal = (safe_mean * danger_std + danger_mean * safe_std) / (safe_std + danger_std)
    else:
        optimal = (safe_mean + danger_mean) / 2
    
    if verbose:
        print("\n" + "="*60)
        print(" ğŸ¯ å‹•çš„é–¾å€¤è¨ˆç®—")
        print("="*60)
        print(f"å®‰å…¨ã‚µãƒ³ãƒ—ãƒ«: mean={safe_mean:.4f}, std={safe_std:.4f}")
        print(f"ç ´æ–­ã‚µãƒ³ãƒ—ãƒ«: mean={danger_mean:.4f}, std={danger_std:.4f}")
        print(f"\né–¾å€¤è¨­å®š:")
        print(f"  å®‰å…¨å´ä¸Šé™ (mean+2Ïƒ): {safe_upper:.4f}")
        print(f"  å±é™ºå´ä¸‹é™ (mean-2Ïƒ): {danger_lower:.4f}")
        print(f"  æœ€é©åˆ†é›¢ç‚¹ (Fisher): {optimal:.4f}")
    
    return ThresholdConfig(
        safe_upper=safe_upper,
        danger_lower=danger_lower,
        optimal=optimal,
        method="dynamic_2sigma"
    )

# =============================================================================
# Section 3: ç›¸å¯¾è·é›¢åˆ¤å®šï¼ˆå±é™ºå¤šæ§˜ä½“ã‚‚ä½¿ç”¨ï¼‰
# =============================================================================

def dual_manifold_classification(
    Lambda: jnp.ndarray,
    safe_manifold: SafeManifold,
    danger_manifold: Optional[DangerManifold] = None,
    weights: Optional[Dict[str, float]] = None
) -> Dict:
    """
    å®‰å…¨/å±é™ºå¤šæ§˜ä½“ã®ä¸¡æ–¹ã‹ã‚‰ã®ç›¸å¯¾è·é›¢ã§åˆ¤å®š
    
    Returns:
        dict: {
            'safe_distance': å®‰å…¨å¤šæ§˜ä½“ã¸ã®è·é›¢,
            'danger_distance': å±é™ºå¤šæ§˜ä½“ã¸ã®è·é›¢ï¼ˆã‚ã‚Œã°ï¼‰,
            'relative_score': ç›¸å¯¾ã‚¹ã‚³ã‚¢ï¼ˆ0=å®‰å…¨ã€1=å±é™ºï¼‰,
            'prediction': 0 or 1,
            'confidence': ç¢ºä¿¡åº¦
        }
    """
    Lambda_smooth = smooth_signal_jax(Lambda, window_size=11)
    G_test = compute_gram(Lambda_smooth)
    
    # å®‰å…¨å¤šæ§˜ä½“ã¸ã®è·é›¢
    safe_distances = batch_gram_distance(G_test, safe_manifold.grams)
    safe_dist = float(jnp.min(safe_distances))
    
    result = {
        'safe_distance': safe_dist,
        'danger_distance': None,
        'relative_score': None,
        'prediction': None,
        'confidence': None
    }
    
    if danger_manifold is not None:
        # å±é™ºå¤šæ§˜ä½“ã¸ã®è·é›¢
        danger_distances = batch_gram_distance(G_test, danger_manifold.grams)
        danger_dist = float(jnp.min(danger_distances))
        result['danger_distance'] = danger_dist
        
        # ç›¸å¯¾ã‚¹ã‚³ã‚¢ï¼ˆ0ã«è¿‘ã„=å®‰å…¨ã€1ã«è¿‘ã„=å±é™ºï¼‰
        relative_score = safe_dist / (safe_dist + danger_dist + 1e-8)
        result['relative_score'] = relative_score
        
        # åˆ¤å®šï¼ˆ0.5ã‚’å¢ƒç•Œï¼‰
        result['prediction'] = 1 if relative_score > 0.5 else 0
        
        # ç¢ºä¿¡åº¦ï¼ˆ0.5ã‹ã‚‰ã®è·é›¢ï¼‰
        result['confidence'] = abs(relative_score - 0.5) * 2
    else:
        # å®‰å…¨å¤šæ§˜ä½“ã®ã¿ã®å ´åˆã¯è·é›¢ãƒ™ãƒ¼ã‚¹
        # çµ±è¨ˆçš„ãªé–¾å€¤ãŒå¿…è¦ï¼ˆåˆ¥é€”è¨ˆç®—ï¼‰
        pass
    
    return result

# =============================================================================
# Section 4: å¤šæ§˜ä½“ãƒ™ãƒ¼ã‚¹ã®æå¤±é–¢æ•°
# =============================================================================

def loss_binary_manifold(
    params,
    exps: List[ExpBinary],
    mat_dict: Dict,
    safe_manifold: SafeManifold,
    danger_manifold: Optional[DangerManifold] = None,
    thresholds: Optional[ThresholdConfig] = None,
    weights: Optional[Dict[str, float]] = None
):
    """
    å¤šæ§˜ä½“ãƒ™ãƒ¼ã‚¹ã®ãƒã‚¤ãƒŠãƒªæå¤±é–¢æ•°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    """
    if weights is None:
        weights = {
            'tv': 0.1,
            'jump': 0.5,
            'topo': 0.1,
            'l1': 1e-3
        }
    
    edr_dict = transform_params_jax(params)
    analyzer = ManifoldAnalyzer(weights)
    
    total_loss = 0.0
    
    if danger_manifold is not None:
        # ç›¸å¯¾è·é›¢ãƒ™ãƒ¼ã‚¹ã®æå¤±
        for exp in exps:
            schedule_dict = schedule_to_jax_dict(exp.schedule)
            res = simulate_lambda_jax(schedule_dict, mat_dict, edr_dict)
            Lambda = res["Lambda"]
            
            result = dual_manifold_classification(
                Lambda, safe_manifold, danger_manifold, weights
            )
            relative_score = result['relative_score']
            
            if exp.failed == 1:
                # ç ´æ–­: relative_scoreã¯1ã«è¿‘ã„ã¹ã
                loss = (1.0 - relative_score)**2
            else:
                # å®‰å…¨: relative_scoreã¯0ã«è¿‘ã„ã¹ã
                loss = relative_score**2
            
            total_loss += loss
    else:
        # å®‰å…¨å¤šæ§˜ä½“ã®ã¿ã®å ´åˆï¼ˆå¾“æ¥ç‰ˆï¼‰
        if thresholds is None:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤
            safe_threshold = 0.25
            danger_threshold = 0.35
        else:
            safe_threshold = thresholds.safe_upper
            danger_threshold = thresholds.danger_lower
        
        for exp in exps:
            schedule_dict = schedule_to_jax_dict(exp.schedule)
            res = simulate_lambda_jax(schedule_dict, mat_dict, edr_dict)
            Lambda = res["Lambda"]
            
            score = analyzer.compute_safety_score(Lambda, safe_manifold)
            
            if exp.failed == 1:
                loss = jnp.maximum(0.0, danger_threshold - score)**2
            else:
                loss = jnp.maximum(0.0, score - safe_threshold)**2
            
            total_loss += loss
    
    return total_loss / len(exps)

# =============================================================================
# Section 5: Phase 0 - æ•™å¸«ãªã—ç‰©ç†åˆ¶ç´„å­¦ç¿’
# =============================================================================

def phase0_unsupervised_learning(
    mat_dict: Dict,
    n_steps: int = 300,
    verbose: bool = True
) -> Tuple[Dict, List[float]]:
    """
    Phase 0: ç‰©ç†åˆ¶ç´„ã®ã¿ã§FLCé¢ã‚’äº‹å‰å­¦ç¿’
    """
    if verbose:
        print("\n" + "="*60)
        print(" ğŸ‚ Phase 0: Unsupervised FLC Manifold Learning")
        print("="*60)
    
    # å®‰å®šç‰ˆFLCäºˆæ¸¬
    def predict_flc_stable(path_ratio, edr_dict):
        duration = 1.0
        major_rate = 0.6
        dt = 1e-3
        N = int(duration/dt) + 1
        t = jnp.linspace(0, duration, N)
        epsM = major_rate * t
        epsm = path_ratio * epsM
        
        schedule_dict = {
            't': t,
            'eps_maj': epsM,
            'eps_min': epsm,
            'triax': jnp.full(N, triax_from_path_jax(path_ratio)),
            'mu': jnp.full(N, 0.08),
            'pN': jnp.full(N, 200e6),
            'vslip': jnp.full(N, 0.02),
            'htc': jnp.full(N, 8000.0),
            'Tdie': jnp.full(N, 293.15),
            'contact': jnp.full(N, 1.0),
            'T0': 293.15
        }
        
        res = simulate_lambda_jax(schedule_dict, mat_dict, edr_dict)
        Lambda_smooth = smooth_signal_jax(res["Lambda"], window_size=11)
        
        exceed_mask = Lambda_smooth > edr_dict['Lambda_crit']
        first_exceed = jnp.argmax(exceed_mask)
        has_exceeded = jnp.any(exceed_mask)
        
        epsM_trimmed = epsM[:-1]
        Em = jnp.where(has_exceeded, epsM_trimmed[first_exceed], epsM_trimmed[-1])
        
        return Em
    
    # Phase 0æå¤±é–¢æ•°
    def loss_phase0(raw_params):
        edr_dict = transform_params_jax(raw_params)
        
        beta_grid = jnp.linspace(-0.8, 0.8, 13)
        
        Em_grid = []
        for beta in beta_grid:
            Em = predict_flc_stable(beta, edr_dict)
            Em = jnp.where(jnp.isnan(Em), 0.3, Em)
            Em = jnp.clip(Em, 0.1, 0.8)
            Em_grid.append(Em)
        
        Em_array = jnp.array(Em_grid)
        
        # ç‰©ç†åˆ¶ç´„
        monotonicity_loss = jnp.mean(jnp.maximum(0, -jnp.diff(jnp.abs(Em_array))))
        
        center = len(beta_grid) // 2
        left_branch = Em_array[:center]
        right_branch = Em_array[center:]
        convexity_loss = jnp.mean(jnp.maximum(0, jnp.diff(left_branch))) + \
                         jnp.mean(jnp.maximum(0, -jnp.diff(right_branch)))
        
        asymmetry_factor = jnp.clip(edr_dict['beta_A_pos'] / (edr_dict['beta_A'] + 1e-8), 0.5, 2.0)
        symmetry_target = Em_array[::-1] * asymmetry_factor
        symmetry_loss = 0.1 * jnp.mean((Em_array - symmetry_target)**2)
        
        grad2 = jnp.diff(jnp.diff(Em_array))
        smoothness_loss = 0.05 * jnp.mean(grad2**2)
        
        range_loss = jnp.mean(jnp.maximum(0, 0.1 - Em_array)**2) + \
                     jnp.mean(jnp.maximum(0, Em_array - 1.0)**2)
        
        total_loss = monotonicity_loss + convexity_loss + symmetry_loss + \
                    smoothness_loss + range_loss
        
        return jnp.where(jnp.isnan(total_loss), 1e10, total_loss)
    
    # åˆæœŸåŒ–ã¨æœ€é©åŒ–
    params_phase0 = init_edr_params_jax()
    
    schedule_phase0 = optax.exponential_decay(
        init_value=3e-3,
        transition_steps=50,
        decay_rate=0.92
    )
    
    optimizer = optax.chain(
        optax.clip_by_global_norm(1.0),
        optax.adam(learning_rate=schedule_phase0)
    )
    
    opt_state = optimizer.init(params_phase0)
    grad_fn = jax.grad(loss_phase0)
    
    loss_history = []
    
    for step in range(n_steps):
        grads = grad_fn(params_phase0)
        updates, opt_state = optimizer.update(grads, opt_state, params_phase0)
        params_phase0 = optax.apply_updates(params_phase0, updates)
        
        if step % 100 == 0:
            loss = float(loss_phase0(params_phase0))
            loss_history.append(loss)
            if verbose:
                print(f"  Step {step:3d}: Physics Loss = {loss:.6f}")
    
    final_loss = float(loss_phase0(params_phase0))
    loss_history.append(final_loss)
    
    if verbose:
        print(f"\n  âœ… Phase 0å®Œäº†: Physics Loss = {final_loss:.6f}")
    
    return params_phase0, loss_history

# =============================================================================
# Section 6: Phase 1.5B - åˆ¶ç´„ä»˜ãå¤šæ§˜ä½“æœ€é©åŒ–
# =============================================================================

def phase_15b_manifold_optimization(
    params_init,
    flc_pts_data: Dict,
    exps: List[ExpBinary],
    mat_dict: Dict,
    safe_manifold: SafeManifold,
    danger_manifold: Optional[DangerManifold] = None,
    flc_target: float = None,
    n_steps: int = 500,
    verbose: bool = True
) -> Tuple[Dict, Dict]:
    """
    Phase 1.5B: FLCåˆ¶ç´„ä»˜ãå¤šæ§˜ä½“ãƒ™ãƒ¼ã‚¹Binaryæœ€é©åŒ–
    """
    # å‹•çš„é–¾å€¤è¨ˆç®—
    edr_dict_init = transform_params_jax(params_init)
    thresholds = calculate_dynamic_thresholds(
        exps, mat_dict, edr_dict_init, safe_manifold, verbose=verbose
    )
    
    # FLCç›®æ¨™å€¤ã®è¨­å®š
    if flc_target is None:
        flc_target = float(jax.device_get(
            loss_flc_true_jax(params_init, flc_pts_data, mat_dict)
        ))
    
    if verbose:
        print("\n" + "="*60)
        print(" ğŸ‚ Phase 1.5B: åˆ¶ç´„ä»˜ãå¤šæ§˜ä½“æœ€é©åŒ–")
        print("="*60)
        print(f"  FLCåˆ¶ç´„: < {flc_target * 1.03:.6f} (3%è¨±å®¹)")
        print(f"  ç›®çš„: Binaryæœ€å°åŒ–ï¼ˆå¤šæ§˜ä½“ãƒ™ãƒ¼ã‚¹ï¼‰")
        
        if danger_manifold is not None:
            print("  âš ï¸  å±é™ºå¤šæ§˜ä½“ã‚‚ä½¿ç”¨ï¼ˆç›¸å¯¾è·é›¢åˆ¤å®šï¼‰")
    
    # åˆ¶ç´„ä»˜ãæå¤±é–¢æ•°
    def loss_constrained(params):
        flc_loss = loss_flc_true_jax(params, flc_pts_data, mat_dict)
        
        bin_loss = loss_binary_manifold(
            params, exps, mat_dict, safe_manifold, 
            danger_manifold, thresholds
        )
        
        # FLCé–¾å€¤ã¨å‹•çš„é‡ã¿ä»˜ã‘
        flc_threshold = flc_target * 1.03
        flc_margin = (flc_loss - flc_threshold) / (flc_threshold * 0.01)
        w_transition = jax.nn.sigmoid(flc_margin * 5.0)
        w_flc = 0.1 + 0.8 * w_transition
        w_bin = 1.0 - w_flc
        
        return w_flc * flc_loss + w_bin * bin_loss, flc_loss, bin_loss
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
    optimizer = optax.chain(
        optax.clip_by_global_norm(0.3),
        optax.adamw(learning_rate=1e-3, weight_decay=1e-5)
    )
    
    opt_state = optimizer.init(params_init)
    params = params_init
    
    grad_fn = jax.grad(lambda p: loss_constrained(p)[0])
    
    history = {
        'total': [],
        'flc': [],
        'binary': []
    }
    
    for step in range(n_steps):
        grads = grad_fn(params)
        updates, opt_state = optimizer.update(grads, opt_state, params)
        params = optax.apply_updates(params, updates)
        
        if step % 100 == 0 and verbose:
            total_loss, flc_loss, bin_loss = loss_constrained(params)
            
            # JAXãƒˆãƒ¬ãƒ¼ã‚¹å€¤ã‚’å…·ä½“å€¤ã«å¤‰æ›
            total_val = float(jax.device_get(total_loss))
            flc_val = float(jax.device_get(flc_loss))
            bin_val = float(jax.device_get(bin_loss))
            
            history['total'].append(total_val)
            history['flc'].append(flc_val)
            history['binary'].append(bin_val)
            
            print(f"  Step {step:3d}: Total = {total_val:.6f} "
                  f"(FLC: {flc_val:.6f}, Binary: {bin_val:.6f})")
        
    if verbose:
        print(f"\n  âœ… Phase 1.5Bå®Œäº†ï¼")
    
    return params, history

# =============================================================================
# Section 7: æ¤œè¨¼ã¨è©•ä¾¡
# =============================================================================

def evaluate_binary_classification(
    exps: List[ExpBinary],
    mat_dict: Dict,
    edr_dict: Dict,
    safe_manifold: SafeManifold,
    danger_manifold: Optional[DangerManifold] = None,
    verbose: bool = True
) -> Dict:
    """
    ãƒã‚¤ãƒŠãƒªåˆ†é¡ã®æ€§èƒ½è©•ä¾¡
    """
    if verbose:
        print("\n" + "="*60)
        print(" ğŸ“Š Binary Classification Evaluation")
        print("="*60)
    
    correct = 0
    results = []
    
    for i, exp in enumerate(exps):
        schedule_dict = schedule_to_jax_dict(exp.schedule)
        res = simulate_lambda_jax(schedule_dict, mat_dict, edr_dict)
        Lambda = res["Lambda"]
        
        # ç›¸å¯¾è·é›¢åˆ¤å®š
        classification = dual_manifold_classification(
            Lambda, safe_manifold, danger_manifold
        )
        
        if danger_manifold is not None:
            pred = classification['prediction']
            conf = classification['confidence']
            
            if verbose:
                print(f"\nExp{i} ({exp.label}):")
                print(f"  çœŸå€¤: {'ç ´æ–­' if exp.failed == 1 else 'å®‰å…¨'}")
                print(f"  äºˆæ¸¬: {'ç ´æ–­' if pred == 1 else 'å®‰å…¨'}")
                print(f"  å®‰å…¨è·é›¢: {classification['safe_distance']:.4f}")
                print(f"  å±é™ºè·é›¢: {classification['danger_distance']:.4f}")
                print(f"  ç›¸å¯¾ã‚¹ã‚³ã‚¢: {classification['relative_score']:.4f}")
                print(f"  ç¢ºä¿¡åº¦: {conf:.2%}")
            
            if pred == exp.failed:
                correct += 1
                if verbose:
                    print("  âœ“ æ­£è§£ï¼")
            else:
                if verbose:
                    print("  âœ— ä¸æ­£è§£")
        
        results.append({
            'exp': exp,
            'classification': classification,
            'correct': pred == exp.failed if danger_manifold else None
        })
    
    accuracy = correct / len(exps) * 100 if danger_manifold else None
    
    if verbose and accuracy is not None:
        print(f"\n{'='*60}")
        print(f"æœ€çµ‚ç²¾åº¦: {accuracy:.2f}%")
        print(f"æ­£è§£æ•°: {correct}/{len(exps)}")
    
    return {
        'accuracy': accuracy,
        'correct': correct,
        'total': len(exps),
        'results': results
    }

# =============================================================================
# Section 8: çµæœã®ä¿å­˜ã¨å¯è¦–åŒ–
# =============================================================================

import json
import pickle
import matplotlib.pyplot as plt
from datetime import datetime

def save_optimization_results(
    edr_params_raw: Dict,
    edr_params: EDRParams,
    flc_results: Optional[Dict] = None,
    binary_results: Optional[Dict] = None,
    history: Optional[Dict] = None,
    output_dir: str = "./results",
    prefix: str = None
) -> str:
    """
    æœ€é©åŒ–çµæœã‚’ä¿å­˜
    
    Args:
        edr_params_raw: ç”Ÿã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆJAX dictï¼‰
        edr_params: EDRParamsãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
        flc_results: FLCãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœ
        binary_results: ãƒã‚¤ãƒŠãƒªåˆ†é¡çµæœ
        history: æœ€é©åŒ–å±¥æ­´
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        prefix: ãƒ•ã‚¡ã‚¤ãƒ«åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
    
    Returns:
        ä¿å­˜å…ˆãƒ‘ã‚¹
    """
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if prefix is None:
        prefix = f"edr_marie_{timestamp}"
    
    # EDRãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¾æ›¸å½¢å¼ã«
    edr_dict = {
        'V0': edr_params.V0,
        'av': edr_params.av,
        'ad': edr_params.ad,
        'chi': edr_params.chi,
        'K_scale': edr_params.K_scale,
        'triax_sens': edr_params.triax_sens,
        'Lambda_crit': edr_params.Lambda_crit,
        'K_scale_draw': edr_params.K_scale_draw,
        'K_scale_plane': edr_params.K_scale_plane,
        'K_scale_biax': edr_params.K_scale_biax,
        'beta_A': edr_params.beta_A,
        'beta_bw': edr_params.beta_bw,
        'beta_A_pos': edr_params.beta_A_pos,
    }
    
    # çµæœã‚’ã¾ã¨ã‚ã‚‹
    results = {
        'timestamp': timestamp,
        'edr_params': edr_dict,
        'edr_params_raw': {k: float(v) for k, v in edr_params_raw.items()},
        'flc_results': flc_results,
        'binary_results': binary_results,
        'history': history
    }
    
    # JSONå½¢å¼ã§ä¿å­˜ï¼ˆäººé–“ãŒèª­ã‚ã‚‹ï¼‰
    json_path = os.path.join(output_dir, f"{prefix}.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # Pickleå½¢å¼ã§ã‚‚ä¿å­˜ï¼ˆå®Œå…¨ãªå†ç¾æ€§ï¼‰
    pkl_path = os.path.join(output_dir, f"{prefix}.pkl")
    with open(pkl_path, 'wb') as f:
        pickle.dump({
            'edr_params_raw': edr_params_raw,
            'edr_params_obj': edr_params,
            'full_results': results
        }, f)
    
    print(f"\nâœ… çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ:")
    print(f"  JSON: {json_path}")
    print(f"  Pickle: {pkl_path}")
    
    return json_path

def plot_flc_fitting_results(
    edr_dict: Dict,
    mat_dict: Dict,
    flc_pts_data: Optional[Dict] = None,
    output_path: Optional[str] = None
) -> plt.Figure:
    """
    FLCãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã‚’å¯è¦–åŒ–
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # FLCæ›²ç·šã‚’è¨ˆç®—
    beta_range = np.linspace(-0.8, 0.8, 41)
    Em_pred = []
    em_pred = []
    
    for beta in beta_range:
        Em, em, _ = predict_flc_from_sim_jax(beta, mat_dict, edr_dict)
        Em_pred.append(float(Em))
        em_pred.append(float(em))
    
    # ãƒ—ãƒ­ãƒƒãƒˆ1: ã²ãšã¿ç©ºé–“ã®FLC
    ax1.plot(em_pred, Em_pred, 'b-', linewidth=2, label='Predicted FLC')
    
    if flc_pts_data is not None:
        ax1.scatter(flc_pts_data['minor_limits'], 
                   flc_pts_data['major_limits'],
                   c='red', s=100, marker='o', label='Experimental', zorder=5)
    
    ax1.set_xlabel('Minor Strain Îµâ‚‚')
    ax1.set_ylabel('Major Strain Îµâ‚')
    ax1.set_title('Forming Limit Curve')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    ax1.set_xlim(-0.3, 0.3)
    ax1.set_ylim(0, 0.5)
    
    # ãƒ—ãƒ­ãƒƒãƒˆ2: Î² vs é™ç•Œä¸»ã²ãšã¿
    ax2.plot(beta_range, Em_pred, 'b-', linewidth=2, label='Predicted')
    
    if flc_pts_data is not None:
        ax2.scatter(flc_pts_data['path_ratios'],
                   flc_pts_data['major_limits'],
                   c='red', s=100, marker='o', label='Experimental', zorder=5)
    
    ax2.set_xlabel('Path Ratio Î²')
    ax2.set_ylabel('Major Strain Limit')
    ax2.set_title('V-shape Profile')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    ax2.set_xlim(-1.0, 1.0)
    ax2.set_ylim(0, 0.5)
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"  FLCå›³ã‚’ä¿å­˜: {output_path}")
    
    return fig

def generate_report(
    edr_params: EDRParams,
    flc_error: Optional[float] = None,
    binary_accuracy: Optional[float] = None,
    phase0_loss: Optional[float] = None,
    final_losses: Optional[Dict] = None,
    output_path: Optional[str] = None
) -> str:
    """
    æœ€é©åŒ–çµæœã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    """
    report = []
    report.append("="*80)
    report.append(" EDR Parameter Fitting Report")
    report.append(" Operation Marie Antoinette Enhanced")
    report.append("="*80)
    report.append(f"\nç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # EDRãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    report.append("ã€EDRãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‘")
    report.append(f"  V0 (åŸºæº–å‡é›†ã‚¨ãƒãƒ«ã‚®ãƒ¼): {edr_params.V0:.2e} Pa")
    report.append(f"  av (ç©ºå­”å½±éŸ¿ä¿‚æ•°): {edr_params.av:.2e}")
    report.append(f"  ad (è»¢ä½å½±éŸ¿ä¿‚æ•°): {edr_params.ad:.2e}")
    report.append(f"  chi (æ‘©æ“¦ç™ºç†±åˆ†é…ç‡): {edr_params.chi:.4f}")
    report.append(f"  K_scale (ç·é‡ã‚¹ã‚±ãƒ¼ãƒ«): {edr_params.K_scale:.4f}")
    report.append(f"  triax_sens (ä¸‰è»¸åº¦æ„Ÿåº¦): {edr_params.triax_sens:.4f}")
    report.append(f"  Lambda_crit (è‡¨ç•ŒÎ›): {edr_params.Lambda_crit:.4f}")
    report.append("")
    report.append("  çµŒè·¯åˆ¥ã‚¹ã‚±ãƒ¼ãƒ«:")
    report.append(f"    æ·±çµã‚Š (draw): {edr_params.K_scale_draw:.4f}")
    report.append(f"    å¹³é¢ã²ãšã¿ (plane): {edr_params.K_scale_plane:.4f}")
    report.append(f"    ç­‰äºŒè»¸ (biax): {edr_params.K_scale_biax:.4f}")
    report.append("")
    report.append("  FLCå½¢çŠ¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
    report.append(f"    beta_A (è°·ã®æ·±ã•): {edr_params.beta_A:.4f}")
    report.append(f"    beta_bw (è°·ã®å¹…): {edr_params.beta_bw:.4f}")
    report.append(f"    beta_A_pos (ç­‰äºŒè»¸å´): {edr_params.beta_A_pos:.4f}")
    report.append("")
    
    # æ€§èƒ½æŒ‡æ¨™
    if any([flc_error, binary_accuracy, phase0_loss, final_losses]):
        report.append("ã€æ€§èƒ½æŒ‡æ¨™ã€‘")
        
        if phase0_loss is not None:
            report.append(f"  Phase 0 ç‰©ç†æå¤±: {phase0_loss:.6f}")
        
        if flc_error is not None:
            report.append(f"  FLCå¹³å‡èª¤å·®: {flc_error:.4f} ({flc_error*100:.2f}%)")
            if flc_error < 0.05:
                report.append("    â†’ âœ… å„ªç§€ (<5%)")
            elif flc_error < 0.10:
                report.append("    â†’ ğŸŸ¡ è‰¯å¥½ (<10%)")
            else:
                report.append("    â†’ ğŸ”´ è¦æ”¹å–„ (>10%)")
        
        if binary_accuracy is not None:
            report.append(f"  ãƒã‚¤ãƒŠãƒªåˆ†é¡ç²¾åº¦: {binary_accuracy:.2f}%")
            if binary_accuracy >= 95:
                report.append("    â†’ âœ… å„ªç§€ (â‰¥95%)")
            elif binary_accuracy >= 90:
                report.append("    â†’ ğŸŸ¡ è‰¯å¥½ (â‰¥90%)")
            else:
                report.append("    â†’ ğŸ”´ è¦æ”¹å–„ (<90%)")
        
        if final_losses:
            report.append("")
            report.append("  æœ€çµ‚æå¤±:")
            for key, value in final_losses.items():
                report.append(f"    {key}: {value:.6f}")
    
    report.append("")
    report.append("="*80)
    
    report_text = "\n".join(report)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        print(f"  ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_path}")
    
    return report_text

def load_optimization_results(filepath: str) -> Dict:
    """
    ä¿å­˜ã•ã‚ŒãŸçµæœã‚’èª­ã¿è¾¼ã¿
    
    Args:
        filepath: JSONã¾ãŸã¯Pickleãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    
    Returns:
        çµæœè¾æ›¸
    """
    if filepath.endswith('.json'):
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    elif filepath.endswith('.pkl'):
        with open(filepath, 'rb') as f:
            return pickle.load(f)
    else:
        raise ValueError(f"æœªå¯¾å¿œã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {filepath}")

# =============================================================================
# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
# =============================================================================

__all__ = [
    # æå¤±é–¢æ•°
    'loss_single_exp_jax',
    'loss_fn_jax',
    'loss_flc_true_jax',
    'loss_binary_manifold',
    
    # é–¾å€¤è¨ˆç®—
    'ThresholdConfig',
    'calculate_dynamic_thresholds',
    
    # åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ 
    'dual_manifold_classification',
    
    # æœ€é©åŒ–
    'phase0_unsupervised_learning',
    'phase_15b_manifold_optimization',

    # çµæœä¿å­˜
    'save_optimization_results',
    'plot_flc_fitting_results',
    'generate_report',
    'load_optimization_results',
    
    # è©•ä¾¡
    'evaluate_binary_classification',
]
