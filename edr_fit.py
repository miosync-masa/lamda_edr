"""
=============================================================================
EDRãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµ±åˆç‰ˆ v5.2 (JAX + CUDA)
+ Operation Marie Antoinetteï¼ˆé€†å•é¡Œãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼‰
Miosync, Inc. / Inverse-EDR Neural Calibration Engine (IENCE)

ã€æ¦‚è¦ã€‘
æ¿ææˆå½¢ã«ãŠã‘ã‚‹ç ´å£Šäºˆæ¸¬ã®ãŸã‚ã®çµ±ä¸€ç†è«–ï¼ˆEDRç†è«–ï¼‰å®Ÿè£…
- JAXç‰ˆï¼šãƒ¡ã‚¤ãƒ³å®Ÿè£…ï¼ˆCPU/GPUä¸¡å¯¾å¿œã€è‡ªå‹•å¾®åˆ†å¯èƒ½ï¼‰
- CUDAç‰ˆï¼šå¤§é‡ä¸¦åˆ—è©•ä¾¡ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- Operation Marie Antoinetteï¼šé€†å•é¡ŒÃ—å¤šæ§˜ä½“å­¦ç¿’ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

ã€æœ€é©åŒ–æˆ¦ç•¥ã€‘
å¤šæ®µéšHybridæœ€é©åŒ–ï¼š
  Phase 0: Unsupervised FLC Pretrainingï¼ˆç‰©ç†åˆ¶ç´„ã®ã¿ï¼‰
  Phase 1: FLCå½¢çŠ¶ç¢ºç«‹ï¼‹æ®µéšçš„ãƒã‚¤ãƒŠãƒªçµ±åˆ
  Phase 1.5B: åˆ¶ç´„ä»˜ãå¤šæ§˜ä½“æœ€é©åŒ–ï¼ˆNEW!ï¼‰
  Phase 2: L-BFGS-Bï¼ˆå±€æ‰€ç²¾å¯†åŒ–ï¼‰- ã‚ªãƒ—ã‚·ãƒ§ãƒ³

ã€è‘—è€…ã€‘
é£¯æ³‰ çœŸé“ (Masamichi Iizumi)
ç’° (Tamaki) - AI Co-Developer

ã€æ—¥ä»˜ã€‘
2025-01-19 (v5.2: Operation Marie Antoinetteçµ±åˆç‰ˆ)
=============================================================================
"""

import numpy as np
import matplotlib.pyplot as plt
from dataclasses import dataclass
from typing import Callable, List, Dict, Tuple, Optional
from scipy.optimize import minimize, Bounds, differential_evolution
from scipy.signal import savgol_filter
from collections import deque
import time

# JAXé–¢é€£ï¼ˆå¿…é ˆï¼‰
try:
    import jax
    import jax.numpy as jnp
    from jax import jit, vmap, grad
    import optax
    JAX_AVAILABLE = True
    print(f"âœ“ JAXåˆ©ç”¨å¯èƒ½: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ {jax.__version__}")
except ImportError:
    raise ImportError("JAXãŒå¿…è¦ã§ã™: pip install jax jaxlib")

# CUDAé–¢é€£ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
CUDA_AVAILABLE = False
try:
    from numba import cuda, float64, int32
    import math
    CUDA_AVAILABLE = cuda.is_available()
    if CUDA_AVAILABLE:
        print(f"âœ“ CUDAåˆ©ç”¨å¯èƒ½: {cuda.get_current_device().name.decode()}")
    else:
        print("âš ï¸  CUDAç„¡åŠ¹: JAX modeã§å®Ÿè¡Œ")
except ImportError:
    print("âš ï¸  Numbaæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: JAX modeã®ã¿")

# Operation Marie Antoinette
print("ğŸ‚ Operation Marie Antoinette: ãƒ­ãƒ¼ãƒ‰ä¸­...")
try:
    from operation_marie_antoinette import (
        build_safe_manifold,
        compute_safety_score,
        loss_binary_manifold,
        phase_15b_manifold_optimization,
        analyze_safety_scores,
        visualize_safe_manifold
    )
    MARIE_ANTOINETTE_AVAILABLE = True
    print("âœ“ Operation Marie Antoinette: åˆ©ç”¨å¯èƒ½")
except ImportError:
    MARIE_ANTOINETTE_AVAILABLE = False
    print("âš ï¸  Operation Marie Antoinette: æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")

# =============================================================================
# Section 1: ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
# =============================================================================

@dataclass
class MaterialParams:
    """ææ–™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"""
    rho: float = 7800.0      # å¯†åº¦ [kg/m3]
    cp: float = 500.0        # æ¯”ç†± [J/kg/K]
    k: float = 40.0          # ç†±ä¼å°ç‡ [W/m/K]
    thickness: float = 0.0008 # æ¿åš [m]
    sigma0: float = 600e6    # åˆæœŸé™ä¼å¿œåŠ› [Pa]
    n: float = 0.15          # åŠ å·¥ç¡¬åŒ–æŒ‡æ•°
    m: float = 0.02          # é€Ÿåº¦æ„Ÿå—æŒ‡æ•°
    r_value: float = 1.0     # ãƒ©ãƒ³ã‚¯ãƒ•ã‚©ãƒ¼ãƒ‰å€¤

@dataclass
class EDRParams:
    """EDRç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"""
    V0: float = 2e9            # åŸºæº–å‡é›†ã‚¨ãƒãƒ«ã‚®ãƒ¼ [Pa = J/m3]
    av: float = 3e4            # ç©ºå­”å½±éŸ¿ä¿‚æ•°
    ad: float = 1e-7           # è»¢ä½å½±éŸ¿ä¿‚æ•°
    chi: float = 0.1           # æ‘©æ“¦ç™ºç†±ã®å†…éƒ¨åˆ†é…ç‡
    K_scale: float = 0.2       # Kç·é‡ã‚¹ã‚±ãƒ¼ãƒ«
    triax_sens: float = 0.3    # ä¸‰è»¸åº¦æ„Ÿåº¦
    Lambda_crit: float = 1.0   # è‡¨ç•ŒÎ›
    # çµŒè·¯åˆ¥ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°
    K_scale_draw: float = 0.15   # æ·±çµã‚Šç”¨
    K_scale_plane: float = 0.25  # å¹³é¢ã²ãšã¿ç”¨
    K_scale_biax: float = 0.20   # ç­‰äºŒè»¸ç”¨
    # FLC Vå­—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    beta_A: float = 0.35       # è°·ã®æ·±ã•
    beta_bw: float = 0.28      # è°·ã®å¹…
    # éå¯¾ç§°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    beta_A_pos: float = 0.50   # ç­‰äºŒè»¸å´ã®æ·±ã•

@dataclass
class PressSchedule:
    """FEM or å®Ÿé¨“ãƒ­ã‚°ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿"""
    t: np.ndarray                 # æ™‚é–“ [s]
    eps_maj: np.ndarray           # ä¸»ã²ãšã¿
    eps_min: np.ndarray           # å‰¯ã²ãšã¿
    triax: np.ndarray             # ä¸‰è»¸åº¦ Ïƒm/Ïƒeq
    mu: np.ndarray                # æ‘©æ“¦ä¿‚æ•°
    pN: np.ndarray                # æ¥è§¦åœ§ [Pa]
    vslip: np.ndarray             # ã™ã¹ã‚Šé€Ÿåº¦ [m/s]
    htc: np.ndarray               # ç†±ä¼é”ä¿‚æ•° [W/m2/K]
    Tdie: np.ndarray              # é‡‘å‹æ¸©åº¦ [K]
    contact: np.ndarray           # æ¥è§¦ç‡ [0-1]
    T0: float = 293.15            # æ¿ã®åˆæœŸæ¸©åº¦ [K]

@dataclass
class ExpBinary:
    """ç ´æ–­/å®‰å…¨ã®ãƒ©ãƒ™ãƒ«ä»˜ãå®Ÿé¨“"""
    schedule: PressSchedule
    failed: int                   # 1:ç ´æ–­, 0:å®‰å…¨
    label: str = ""

@dataclass
class FLCPoint:
    """FLC: çµŒè·¯æ¯”ä¸€å®šã§ã®é™ç•Œç‚¹ï¼ˆå®Ÿæ¸¬ï¼‰"""
    path_ratio: float            # Î² = eps_min/eps_maj
    major_limit: float           # å®Ÿæ¸¬é™ç•Œä¸»ã²ãšã¿
    minor_limit: float           # å®Ÿæ¸¬é™ç•Œå‰¯ã²ãšã¿
    rate_major: float = 1.0      # ä¸»ã²ãšã¿é€Ÿåº¦ [1/s]
    duration_max: float = 1.0    # è©¦é¨“ä¸Šé™æ™‚é–“ [s]
    label: str = ""

# =============================================================================
# Section 2: JAXãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# =============================================================================

def schedule_to_jax_dict(schedule: PressSchedule):
    """PressSchedule â†’ JAXç”¨dictå¤‰æ›"""
    return {
        't': jnp.array(schedule.t),
        'eps_maj': jnp.array(schedule.eps_maj),
        'eps_min': jnp.array(schedule.eps_min),
        'triax': jnp.array(schedule.triax),
        'mu': jnp.array(schedule.mu),
        'pN': jnp.array(schedule.pN),
        'vslip': jnp.array(schedule.vslip),
        'htc': jnp.array(schedule.htc),
        'Tdie': jnp.array(schedule.Tdie),
        'contact': jnp.array(schedule.contact),
        'T0': schedule.T0
    }

def mat_to_jax_dict(mat: MaterialParams):
    """MaterialParams â†’ JAXç”¨dictå¤‰æ›"""
    return {
        'rho': mat.rho,
        'cp': mat.cp,
        'h0': mat.thickness,
        'sigma0': mat.sigma0,
        'n': mat.n,
        'm': mat.m,
        'r_value': mat.r_value
    }

@jit
def soft_clamp(x, min_val, max_val):
    """ã‚½ãƒ•ãƒˆå¢ƒç•Œåˆ¶ç´„ï¼ˆå¾®åˆ†å¯èƒ½ï¼‰"""
    return min_val + (max_val - min_val) * jax.nn.sigmoid(x)

@jit
def triax_from_path_jax(beta):
    """ã²ãšã¿çµŒè·¯æ¯”Î²ã‹ã‚‰ä¸‰è»¸åº¦Î·ã‚’è¨ˆç®—"""
    b = jnp.clip(beta, -0.95, 1.0)
    return (1.0 + b) / (jnp.sqrt(3.0) * jnp.sqrt(1.0 + b + b*b))

@jit
def equiv_strain_rate_jax(epsM_dot, epsm_dot):
    """ç›¸å½“ã²ãšã¿é€Ÿåº¦"""
    sqrt_2_3 = 0.8164965809277260
    return sqrt_2_3 * jnp.sqrt(
        (epsM_dot - epsm_dot)**2 + epsM_dot**2 + epsm_dot**2
    )

@jit
def flow_stress_jax(ep_eq, epdot_eq, sigma0, n, m, r_value, T):
    """æ¸©åº¦ä¾å­˜ã®æµå‹•å¿œåŠ›"""
    Tref = 293.15
    alpha = 3e-4
    rate_fac = jnp.power(jnp.maximum(epdot_eq, 1e-6), m)
    aniso = (2.0 + r_value) / 3.0
    temp_fac = 1.0 - alpha * jnp.maximum(T - Tref, 0.0)
    return sigma0 * temp_fac * jnp.power(1.0 + ep_eq, n) * rate_fac / aniso

@jit
def step_cv_jax(cv, T, rho_d, dt):
    """ç©ºå­”æ¿ƒåº¦ã®æ™‚é–“ç™ºå±•"""
    kB_eV = 8.617e-5
    c0 = 1e-6; Ev_eV = 1.0; tau0 = 1e-3; Q_eV = 0.8
    k_ann = 1e6; k_sink = 1e-15
    
    cv_eq = c0 * jnp.exp(-Ev_eV / (kB_eV * T))
    tau = tau0 * jnp.exp(Q_eV / (kB_eV * T))
    dcv = (cv_eq - cv) / tau - k_ann * cv**2 - k_sink * cv * rho_d
    return cv + dcv * dt

@jit
def step_rho_jax(rho_d, epdot_eq, T, dt):
    """è»¢ä½å¯†åº¦ã®æ™‚é–“ç™ºå±•"""
    A = 1e14; B = 1e-4; Qv_eV = 0.8; kB_eV = 8.617e-5
    Dv = 1e-6 * jnp.exp(-Qv_eV / (kB_eV * T))
    drho = A * jnp.maximum(epdot_eq, 0.0) - B * rho_d * Dv
    return jnp.maximum(rho_d + drho * dt, 1e10)

@jit
def get_k_scale_smooth_jax(beta, params):
    """æ»‘ã‚‰ã‹ãªK_scaleé¸æŠï¼ˆåˆ†å²ãƒ¬ã‚¹ï¼‰"""
    w_draw = jnp.exp(-((beta + 0.5) / 0.1)**2)
    w_plane = jnp.exp(-(beta / 0.1)**2)
    w_biax = jnp.exp(-((beta - 0.5) / 0.2)**2)
    w_else = 1.0 - jnp.maximum(w_draw, jnp.maximum(w_plane, w_biax))
    
    w_sum = w_draw + w_plane + w_biax + w_else + 1e-8
    
    return (params["K_scale_draw"] * w_draw + 
            params["K_scale_plane"] * w_plane +
            params["K_scale_biax"] * w_biax +
            params["K_scale"] * w_else) / w_sum

@jit
def beta_multiplier_jax(beta, A, bw):
    """Î²ä¾å­˜ã‚²ã‚¤ãƒ³ï¼ˆVå­—å½¢çŠ¶ï¼‰"""
    b = jnp.clip(beta, -0.95, 0.95)
    return 1.0 + A * jnp.exp(-(b / bw)**2)

@jit
def beta_multiplier_asymmetric_jax(beta, A_neg, A_pos, bw):
    """éå¯¾ç§°Î²ä¾å­˜ã‚²ã‚¤ãƒ³"""
    b = jnp.clip(beta, -0.95, 0.95)
    A = jnp.where(b < 0, A_neg, A_pos)
    return 1.0 + A * jnp.exp(-(b / bw)**2)

@jit
def mu_effective_jax(mu0, T, pN, vslip):
    """æ¸©åº¦ãƒ»é€Ÿåº¦ãƒ»è·é‡ä¾å­˜ã®æœ‰åŠ¹æ‘©æ“¦ä¿‚æ•°ï¼ˆStribecké¢¨ï¼‰"""
    s = (vslip * 1e3) / (pN / 1e6 + 1.0)
    stribeck = 0.7 + 0.3 / (1 + s)
    temp_reduction = 1.0 - 1e-4 * jnp.maximum(T - 293.15, 0)
    return mu0 * stribeck * temp_reduction

def smooth_signal_jax(x, window_size=11):
    """JAXç‰ˆç§»å‹•å¹³å‡ã«ã‚ˆã‚‹ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆã‚¹ãƒ‘ã‚¤ã‚¯é™¤å»ï¼‰"""
    if window_size <= 1 or len(x) <= window_size:
        return x
    kernel = jnp.ones(window_size) / window_size
    # JAXç‰ˆã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¨ç•³ã¿è¾¼ã¿
    padded = jnp.pad(x, (window_size//2, window_size//2), mode='edge')
    smoothed = jnp.convolve(padded, kernel, mode='valid')
    return smoothed[:len(x)]

# =============================================================================
# Section 3: ãƒ¡ã‚¤ãƒ³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆJAXç‰ˆï¼‰
# =============================================================================

@jit
def simulate_lambda_jax(schedule_dict, mat_dict, edr_dict):
    """JAXç‰ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ¡ã‚¤ãƒ³å®Ÿè£…ï¼‰"""
    
    # ãƒ‡ãƒ¼ã‚¿å–ã‚Šå‡ºã—
    t = schedule_dict['t']
    epsM = schedule_dict['eps_maj']
    epsm = schedule_dict['eps_min']
    triax = schedule_dict['triax']
    mu = schedule_dict['mu']
    pN = schedule_dict['pN']
    vslip = schedule_dict['vslip']
    htc = schedule_dict['htc']
    Tdie = schedule_dict['Tdie']
    contact = schedule_dict['contact']
    T0 = schedule_dict['T0']
    
    dt = (t[-1] - t[0]) / (len(t) - 1)
    
    # ã²ãšã¿é€Ÿåº¦è¨ˆç®—
    epsM_dot = jnp.gradient(epsM, dt)
    epsm_dot = jnp.gradient(epsm, dt)
    
    # çµŒè·¯å¹³å‡Î²
    beta_avg = jnp.mean(epsm / (epsM + 1e-10))
    
    # scanã§æ™‚é–“ãƒ«ãƒ¼ãƒ—
    def time_step(carry, inputs):
        T, cv, rho_d, ep_eq, h_eff, eps3, beta_hist = carry
        idx = inputs
        
        epsM_dot_t = epsM_dot[idx]
        epsm_dot_t = epsm_dot[idx]
        triax_t = triax[idx]
        mu_t = mu[idx]
        pN_t = pN[idx]
        vslip_t = vslip[idx]
        htc_t = htc[idx]
        Tdie_t = Tdie[idx]
        contact_t = contact[idx]
        
        # ç›¸å½“ã²ãšã¿é€Ÿåº¦
        epdot_eq = equiv_strain_rate_jax(epsM_dot_t, epsm_dot_t)
        
        # æ¿åšæ›´æ–°
        d_eps3 = -(epsM_dot_t + epsm_dot_t) * dt
        eps3_new = eps3 + d_eps3
        h_eff_new = jnp.maximum(mat_dict['h0'] * jnp.exp(eps3_new), 0.2 * mat_dict['h0'])
        
        # ç†±åæ”¯
        q_fric = mu_t * pN_t * vslip_t * contact_t
        dTdt = (2.0 * htc_t * (Tdie_t - T) + 2.0 * edr_dict['chi'] * q_fric) / \
               (mat_dict['rho'] * mat_dict['cp'] * h_eff_new)
        dTdt = jnp.clip(dTdt, -1000.0, 1000.0)
        T_new = jnp.clip(T + dTdt * dt, 200.0, 2000.0)
        
        # æ¬ é™¥æ›´æ–°
        rho_d_new = step_rho_jax(rho_d, epdot_eq, T, dt)
        cv_new = step_cv_jax(cv, T, rho_d_new, dt)
        
        # Kè¨ˆç®—ï¼ˆæ”¹å–„ç‰ˆï¼šå†·å´ã¯å›å¾©å´ï¼‰
        K_th = mat_dict['rho'] * mat_dict['cp'] * jnp.maximum(dTdt, 0.0)  # åŠ ç†±æ™‚ã®ã¿ã‚«ã‚¦ãƒ³ãƒˆï¼
        
        # æ¸©åº¦ä¾å­˜ã®æµå‹•å¿œåŠ›
        sigma_eq = flow_stress_jax(ep_eq, epdot_eq, mat_dict['sigma0'], 
                                   mat_dict['n'], mat_dict['m'], mat_dict['r_value'], T)
        K_pl = 0.9 * sigma_eq * epdot_eq
        
        # æ¸©åº¦ãƒ»é€Ÿåº¦ãƒ»è·é‡ä¾å­˜ã®æ‘©æ“¦ä¿‚æ•°ï¼ˆç‰©ç†å¢—å¼·ï¼‰
        mu_eff = mu_effective_jax(mu_t, T, pN_t, vslip_t)
        q_fric_eff = mu_eff * pN_t * vslip_t * contact_t
        K_fr = (2.0 * edr_dict['chi'] * q_fric_eff) / h_eff_new
        
        # K_scaleé¸æŠ
        k_scale_path = get_k_scale_smooth_jax(beta_avg, edr_dict)
        
        # Î²ç¬é–“å€¤ã¨Î²å±¥æ­´ã®5ç‚¹ç§»å‹•å¹³å‡
        beta_inst = epsm_dot_t / (epsM_dot_t + 1e-8)
        
        # Î²å±¥æ­´æ›´æ–°ï¼ˆ5ç‚¹ç§»å‹•å¹³å‡ï¼‰
        beta_hist_new = jnp.roll(beta_hist, -1).at[4].set(beta_inst)
        beta_smooth = jnp.mean(beta_hist_new)
        
        # K_totalï¼ˆéå¯¾ç§°ã‚²ã‚¤ãƒ³ä½¿ç”¨ï¼‰
        K_total = k_scale_path * (K_th + K_pl + K_fr)
        K_total *= beta_multiplier_asymmetric_jax(
            beta_smooth, 
            edr_dict['beta_A'], 
            edr_dict.get('beta_A_pos', edr_dict['beta_A']),
            edr_dict['beta_bw']
        )
        K_total = jnp.maximum(K_total, 0.0)
        K_total = jnp.where(jnp.isnan(K_total), 0.0, K_total)  # NaNå¯¾ç­–
        
        # V_effï¼ˆæ¸©åº¦ä¾å­˜æ€§ã‚’å¼·åŒ–ï¼‰
        T_ratio = jnp.minimum((T - 273.15) / (1500.0 - 273.15), 1.0)
        temp_factor = 1.0 - 0.5 * T_ratio  # æ¸©åº¦ãŒä¸ŠãŒã‚‹ã¨V_effãŒä¸‹ãŒã‚‹
        V_eff = edr_dict['V0'] * temp_factor * \
                (1.0 - edr_dict['av'] * cv - edr_dict['ad'] * jnp.sqrt(jnp.maximum(rho_d, 1e10)))
        V_eff = jnp.maximum(V_eff, 0.01 * edr_dict['V0'])
        V_eff = jnp.where(jnp.isnan(V_eff), edr_dict['V0'], V_eff)  # NaNå¯¾ç­–
        
        # ä¸‰è»¸åº¦è£œæ­£ï¼ˆæ„Ÿåº¦ã‚’èª¿æ•´ï¼‰
        D_triax = jnp.exp(-edr_dict['triax_sens'] * jnp.maximum(triax_t, 0.0))
        D_triax = jnp.where(jnp.isnan(D_triax), 1.0, D_triax)  # NaNå¯¾ç­–
        
        # Î›è¨ˆç®—
        Lambda = K_total / jnp.maximum(V_eff * D_triax, 1e7)
        Lambda = jnp.minimum(Lambda, 10.0)
        Lambda = jnp.where(jnp.isnan(Lambda), 0.0, Lambda)  # NaNå¯¾ç­–
        
        # ç›¸å½“å¡‘æ€§ã²ãšã¿æ›´æ–°
        ep_eq_new = ep_eq + epdot_eq * dt
        
        new_carry = (T_new, cv_new, rho_d_new, ep_eq_new, h_eff_new, eps3_new, beta_hist_new)
        return new_carry, Lambda
    
    # åˆæœŸçŠ¶æ…‹ï¼ˆÎ²å±¥æ­´ã‚‚åˆæœŸåŒ–ï¼‰
    init_beta_hist = jnp.zeros(5)  # 5ç‚¹ç§»å‹•å¹³å‡ç”¨
    init_carry = (T0, 1e-7, 1e11, 0.0, mat_dict['h0'], 0.0, init_beta_hist)
    
    # scanå®Ÿè¡Œ
    indices = jnp.arange(len(t) - 1)
    _, Lambdas = jax.lax.scan(time_step, init_carry, indices)
    
    # Damageç©åˆ†
    Damage = jnp.cumsum(jnp.maximum(Lambdas - edr_dict['Lambda_crit'], 0.0) * dt)
    
    return {"Lambda": Lambdas, "Damage": Damage}

# =============================================================================
# Section 4: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç®¡ç†ã¨æå¤±é–¢æ•°
# =============================================================================

def init_edr_params_jax():
    """JAXç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–ï¼ˆãƒãƒ©ãƒ³ã‚¹èª¿æ•´ç‰ˆï¼‰"""
    return {
        'log_V0': jnp.log(1.5e9),
        'log_av': jnp.log(4e4),
        'log_ad': jnp.log(1e-7),
        'logit_chi': jnp.log(0.09 / (1 - 0.09)),
        'logit_K_scale': jnp.log(0.25 / (1 - 0.25)),
        'logit_K_scale_draw': jnp.log(0.18 / (1 - 0.18)),
        'logit_K_scale_plane': jnp.log(0.25 / (1 - 0.25)),
        'logit_K_scale_biax': jnp.log(0.22 / (1 - 0.22)),
        'logit_triax_sens': jnp.log(0.28 / (1 - 0.28)),
        'Lambda_crit': jnp.array(0.97),
        'logit_beta_A': jnp.log(0.32 / (1 - 0.32)),
        'logit_beta_bw': jnp.log(0.29 / (1 - 0.29)),
        'logit_beta_A_pos': jnp.log(0.48 / (1 - 0.48)),
    }

def transform_params_jax(raw_params):
    """åˆ¶ç´„ä»˜ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›ï¼ˆsoft boundsï¼‰"""
    return {
        'V0': jnp.exp(raw_params['log_V0']),
        'av': jnp.exp(raw_params['log_av']),
        'ad': jnp.exp(raw_params['log_ad']),
        'chi': soft_clamp(raw_params['logit_chi'], 0.05, 0.3),
        'K_scale': soft_clamp(raw_params['logit_K_scale'], 0.05, 1.0),
        'K_scale_draw': soft_clamp(raw_params['logit_K_scale_draw'], 0.05, 0.3),
        'K_scale_plane': soft_clamp(raw_params['logit_K_scale_plane'], 0.1, 0.4),
        'K_scale_biax': soft_clamp(raw_params['logit_K_scale_biax'], 0.05, 0.3),
        'triax_sens': soft_clamp(raw_params['logit_triax_sens'], 0.1, 0.5),
        'Lambda_crit': jnp.clip(raw_params['Lambda_crit'], 0.95, 1.05),
        'beta_A': soft_clamp(raw_params['logit_beta_A'], 0.2, 0.5),
        'beta_bw': soft_clamp(raw_params['logit_beta_bw'], 0.2, 0.35),
        'beta_A_pos': soft_clamp(raw_params['logit_beta_A_pos'], 0.3, 0.7),
    }

def edr_dict_to_dataclass(edr_dict):
    """dict â†’ EDRParamså¤‰æ›ï¼ˆJAXå€¤ã‚’å–å¾—ï¼‰"""
    return EDRParams(
        V0=float(jax.device_get(edr_dict['V0'])),
        av=float(jax.device_get(edr_dict['av'])),
        ad=float(jax.device_get(edr_dict['ad'])),
        chi=float(jax.device_get(edr_dict['chi'])),
        K_scale=float(jax.device_get(edr_dict['K_scale'])),
        triax_sens=float(jax.device_get(edr_dict['triax_sens'])),
        Lambda_crit=float(jax.device_get(edr_dict['Lambda_crit'])),
        K_scale_draw=float(jax.device_get(edr_dict['K_scale_draw'])),
        K_scale_plane=float(jax.device_get(edr_dict['K_scale_plane'])),
        K_scale_biax=float(jax.device_get(edr_dict['K_scale_biax'])),
        beta_A=float(jax.device_get(edr_dict['beta_A'])),
        beta_bw=float(jax.device_get(edr_dict['beta_bw'])),
        beta_A_pos=float(jax.device_get(edr_dict.get('beta_A_pos', edr_dict['beta_A']))),
    )

def loss_single_exp_jax(schedule_dict, mat_dict, edr_dict, failed):
    """å˜ä¸€å®Ÿé¨“ã®æå¤±ï¼ˆJAXç‰ˆï¼‰"""
    res = simulate_lambda_jax(schedule_dict, mat_dict, edr_dict)
    
    # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã—ã¦ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’é™¤å»ï¼ˆé‡è¦ï¼ï¼‰
    Lambda_smooth = smooth_signal_jax(res["Lambda"], window_size=11)
    
    peak = jnp.max(Lambda_smooth)
    D_end = res["Damage"][-1]
    
    margin = 0.08
    Dcrit = 0.01
    delta = 0.03
    
    # failed == 1ã®å ´åˆï¼ˆç ´æ–­ï¼‰
    condition_met = (peak > edr_dict['Lambda_crit']) & (D_end > Dcrit)
    
    peak_penalty = jnp.maximum(0.0, edr_dict['Lambda_crit'] - peak)
    D_penalty = jnp.maximum(0.0, Dcrit - D_end)
    loss_fail_not_met = 10.0 * (peak_penalty**2 + D_penalty**2)
    
    margin_loss = jnp.where(
        peak < edr_dict['Lambda_crit'] + margin,
        (edr_dict['Lambda_crit'] + margin - peak)**2,
        0.0
    )
    D_loss = jnp.where(
        D_end < 2*Dcrit,
        (2*Dcrit - D_end)**2,
        0.0
    )
    loss_fail_met = margin_loss + D_loss
    
    loss_fail = jnp.where(condition_met, loss_fail_met, loss_fail_not_met)
    
    # failed == 0ã®å ´åˆï¼ˆå®‰å…¨ï¼‰
    loss_safe_peak = jnp.where(
        peak > edr_dict['Lambda_crit'] - delta,
        (peak - (edr_dict['Lambda_crit'] - delta))**2 * 3.0,
        0.0
    )
    loss_safe_D = jnp.where(
        D_end >= 0.5*Dcrit,
        10.0 * (D_end - 0.5*Dcrit)**2,
        0.0
    )
    loss_safe = loss_safe_peak + loss_safe_D
    
    return jnp.where(failed == 1, loss_fail, loss_safe)

def loss_fn_jax(raw_params, exps, mat):
    """ãƒãƒƒãƒæå¤±é–¢æ•°"""
    edr_dict = transform_params_jax(raw_params)
    
    # ãƒ‡ãƒ¼ã‚¿å¤‰æ›
    mat_dict = mat_to_jax_dict(mat)
    
    total_loss = 0.0
    for exp in exps:
        schedule_dict = schedule_to_jax_dict(exp.schedule)
        loss = loss_single_exp_jax(schedule_dict, mat_dict, edr_dict, exp.failed)
        total_loss += loss
    
    return total_loss / len(exps)

def predict_flc_from_sim_jax(beta, mat_dict, edr_dict, 
                              major_rate=0.6, duration=1.0):
    """Î›(t)ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰FLCé™ç•Œç‚¹ã‚’å¾®åˆ†å¯èƒ½ã«æŠ½å‡º"""
    dt = 1e-3
    N = int(duration/dt) + 1
    t = jnp.linspace(0, duration, N)
    epsM = major_rate * t
    epsm = beta * epsM
    
    schedule_dict = {
        't': t,
        'eps_maj': epsM,
        'eps_min': epsm,
        'triax': jnp.full(N, triax_from_path_jax(beta)),
        'mu': jnp.full(N, 0.08),
        'pN': jnp.full(N, 200e6),
        'vslip': jnp.full(N, 0.02),
        'htc': jnp.full(N, 8000.0),
        'Tdie': jnp.full(N, 293.15),
        'contact': jnp.full(N, 1.0),
        'T0': 293.15
    }
    
    res = simulate_lambda_jax(schedule_dict, mat_dict, edr_dict)
    Lambda_raw = res["Lambda"]  # é•·ã•ã¯N-1
    
    # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
    Lambda_smooth = smooth_signal_jax(Lambda_raw, window_size=11)
    
    # epsMé…åˆ—ã‚’Lambdaã®é•·ã•ã«åˆã‚ã›ã‚‹ï¼ˆæœ€å¾Œã®è¦ç´ ã‚’é™¤ãï¼‰
    epsM_trimmed = epsM[:-1]
    
    # å¾®åˆ†å¯èƒ½ãªé™ç•Œç‚¹æ¤œå‡º
    exceed = jnp.maximum(Lambda_smooth - edr_dict['Lambda_crit'], 0.0)
    w = jnp.exp(jnp.minimum(10.0 * exceed, 10.0))
    w = w / (jnp.sum(w) + 1e-12)
    Em = jnp.sum(w * epsM_trimmed)
    Em = jnp.where(jnp.isnan(Em), epsM_trimmed[-1], Em)
    em = beta * Em
    
    return Em, em, Lambda_smooth

def loss_flc_true_jax(raw_params, flc_pts_data, mat_dict):
    """çœŸã®Î›ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹FLCæå¤±"""
    edr_dict = transform_params_jax(raw_params)
    
    # FLCç‚¹ã”ã¨ã®èª¤å·®ã‚’é †æ¬¡è¨ˆç®—
    flc_err = 0.0
    for i in range(len(flc_pts_data['path_ratios'])):
        beta = flc_pts_data['path_ratios'][i]
        Em_gt = flc_pts_data['major_limits'][i]
        em_gt = flc_pts_data['minor_limits'][i]
        
        Em_pred, em_pred, _ = predict_flc_from_sim_jax(beta, mat_dict, edr_dict)
        
        # Î²ä¾å­˜ã®é‡ã¿ä»˜ã‘ï¼ˆç­‰äºŒè»¸ã‚’æœ€é‡è¦–ï¼‰
        w = jnp.where(jnp.abs(beta - 0.5) < 0.1, 6.0,
                      jnp.where(jnp.abs(beta) < 0.1, 1.8, 1.0))
        
        flc_err += w * ((Em_pred - Em_gt)**2 + (em_pred - em_gt)**2)
    
    flc_err = flc_err / len(flc_pts_data['path_ratios'])
    
    # Vå­—å½¢çŠ¶ã¨æ»‘ã‚‰ã‹ã•ã®æ­£å‰‡åŒ–
    beta_batch = jnp.linspace(-0.6, 0.6, 21)
    
    # æ­£å‰‡åŒ–ç”¨ã®FLCæ›²ç·šã‚’è¨ˆç®—
    Em_curve = []
    for beta in beta_batch:
        Em, _, _ = predict_flc_from_sim_jax(beta, mat_dict, edr_dict)
        Em_curve.append(Em)
    Em_curve = jnp.array(Em_curve)
    
    # Vå­—å½¢çŠ¶ã®æ­£å‰‡åŒ–
    center_idx = len(beta_batch) // 2
    center = Em_curve[center_idx]
    valley_loss = 0.1 * jnp.mean(jnp.maximum(0.0, Em_curve - center))
    
    # æ›²ç‡ã®æ­£å‰‡åŒ–ï¼ˆæ»‘ã‚‰ã‹ã•ï¼‰
    grad2 = jnp.diff(jnp.diff(Em_curve))
    smoothness_loss = 0.05 * jnp.mean(grad2**2) + 0.02 * jnp.mean(jnp.abs(grad2))
    
    # å‹•çš„é‡ã¿ä»˜ã‘ï¼ˆåˆ†æ•£ã«å¿œã˜ã¦æ­£å‰‡åŒ–ã‚’èª¿æ•´ï¼‰
    valley_weight = jnp.clip(jnp.var(Em_curve), 0.05, 0.3)
    valley_loss = valley_weight * valley_loss
    
    total_loss = flc_err + valley_loss + smoothness_loss
    
    return total_loss

# =============================================================================
# Section 5: å¤šæ®µéšHybridæœ€é©åŒ–
# =============================================================================

def hybrid_staged_optimization(
    exps: List[ExpBinary],
    flc_pts: List[FLCPoint],
    mat: MaterialParams,
    initial_edr: Optional[EDRParams] = None,
    verbose: bool = True
) -> Tuple[EDRParams, Dict]:
    """
    å¤šæ®µéšHybridæœ€é©åŒ–
    Phase 0: Unsupervised FLC Pretrainingï¼ˆç‰©ç†åˆ¶ç´„ã®ã¿ï¼‰
    Phase 1: FLCå½¢çŠ¶ç¢ºç«‹ï¼‹æ®µéšçš„ãƒã‚¤ãƒŠãƒªçµ±åˆ
    Phase 2: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆå¿…è¦ã«å¿œã˜ã¦L-BFGS-Bï¼‰
    """
    
    if initial_edr is None:
        initial_edr = EDRParams()
    
    mat_dict = mat_to_jax_dict(mat)
    
    # ===========================
    # Phase 0: Unsupervised FLC Pretraining
    # ===========================
    if verbose:
        print("\n" + "="*60)
        print(" Phase 0: Unsupervised FLC Manifold Learning")
        print("="*60)
        print("  ç‰©ç†åˆ¶ç´„ã®ã¿ã§FLCé¢ã‚’äº‹å‰å­¦ç¿’")
    
    # Phase 0ç”¨ã®å®‰å®šç‰ˆFLCäºˆæ¸¬
    def predict_flc_jax_stable(path_ratio, edr_dict, mat_dict):
        """Phase 0ç”¨ã®å®‰å®šç‰ˆFLCäºˆæ¸¬ï¼ˆargmaxä½¿ç”¨ï¼‰"""
        duration = 1.0
        major_rate = 0.6
        dt = 1e-3
        N = int(duration/dt) + 1
        t = jnp.linspace(0, duration, N)
        epsM = major_rate * t
        epsm = path_ratio * epsM
        
        schedule_dict = {
            't': t,
            'eps_maj': epsM,
            'eps_min': epsm,
            'triax': jnp.full(N, triax_from_path_jax(path_ratio)),
            'mu': jnp.full(N, 0.08),
            'pN': jnp.full(N, 200e6),
            'vslip': jnp.full(N, 0.02),
            'htc': jnp.full(N, 8000.0),
            'Tdie': jnp.full(N, 293.15),
            'contact': jnp.full(N, 1.0),
            'T0': 293.15
        }
        
        res = simulate_lambda_jax(schedule_dict, mat_dict, edr_dict)
        Lambda_smooth = smooth_signal_jax(res["Lambda"], window_size=11)
        
        # å®‰å®šç‰ˆï¼šargmaxä½¿ç”¨
        exceed_mask = Lambda_smooth > edr_dict['Lambda_crit']
        first_exceed = jnp.argmax(exceed_mask)
        has_exceeded = jnp.any(exceed_mask)
        
        epsM_trimmed = epsM[:-1]
        Em = jnp.where(has_exceeded, epsM_trimmed[first_exceed], epsM_trimmed[-1])
        em = path_ratio * Em
        
        return Em, em
    
    # Phase 0: æ•™å¸«ãªã—FLCé¢å­¦ç¿’
    def loss_phase0(raw_params):
        """ç‰©ç†åˆ¶ç´„ã®ã¿ã§FLCé¢ã‚’å­¦ç¿’"""
        edr_dict = transform_params_jax(raw_params)
        
        beta_grid = jnp.linspace(-0.8, 0.8, 13)
        
        # å„Î²ã§ã®ä»®æƒ³FLCé™ç•Œã‚’è¨ˆç®—
        Em_grid = []
        for beta in beta_grid:
            Em, _ = predict_flc_jax_stable(beta, edr_dict, mat_dict)
            Em = jnp.where(jnp.isnan(Em), 0.3, Em)
            Em = jnp.clip(Em, 0.1, 0.8)
            Em_grid.append(Em)
        
        Em_array = jnp.array(Em_grid)
        Em_array = jnp.where(jnp.isnan(Em_array), 0.3, Em_array)
        
        # ç‰©ç†åˆ¶ç´„1: å˜èª¿æ€§
        monotonicity_loss = jnp.mean(jnp.maximum(0, -jnp.diff(jnp.abs(Em_array))))
        monotonicity_loss = jnp.where(jnp.isnan(monotonicity_loss), 0.0, monotonicity_loss)
        
        # ç‰©ç†åˆ¶ç´„2: å‡¸æ€§ï¼ˆVå­—å½¢çŠ¶ï¼‰
        center = len(beta_grid) // 2
        left_branch = Em_array[:center]
        right_branch = Em_array[center:]
        
        convexity_loss = jnp.mean(jnp.maximum(0, jnp.diff(left_branch))) + \
                         jnp.mean(jnp.maximum(0, -jnp.diff(right_branch)))
        convexity_loss = jnp.where(jnp.isnan(convexity_loss), 0.0, convexity_loss)
        
        # ç‰©ç†åˆ¶ç´„3: å¯¾ç§°æ€§ï¼ˆç ´ã‚Œã‚’è¨±å®¹ï¼‰
        asymmetry_factor = jnp.clip(edr_dict['beta_A_pos'] / (edr_dict['beta_A'] + 1e-8), 0.5, 2.0)
        symmetry_target = Em_array[::-1] * asymmetry_factor
        symmetry_loss = 0.1 * jnp.mean((Em_array - symmetry_target)**2)
        symmetry_loss = jnp.where(jnp.isnan(symmetry_loss), 0.0, symmetry_loss)
        
        # ç‰©ç†åˆ¶ç´„4: å¹³æ»‘æ€§
        grad2 = jnp.diff(jnp.diff(Em_array))
        smoothness_loss = 0.05 * jnp.mean(grad2**2)
        smoothness_loss = jnp.where(jnp.isnan(smoothness_loss), 0.0, smoothness_loss)
        
        # ç‰©ç†åˆ¶ç´„5: åˆç†çš„ãªç¯„å›²
        range_loss = jnp.mean(jnp.maximum(0, 0.1 - Em_array)**2) + \
                     jnp.mean(jnp.maximum(0, Em_array - 1.0)**2)
        range_loss = jnp.where(jnp.isnan(range_loss), 0.0, range_loss)
        
        total_loss = monotonicity_loss + convexity_loss + symmetry_loss + \
                    smoothness_loss + range_loss
        
        total_loss = jnp.where(jnp.isnan(total_loss), 1e10, total_loss)
        
        return total_loss
    
    # Phase 0ã®åˆæœŸåŒ–
    params_phase0 = init_edr_params_jax()
    
    # Phase 0æœ€é©åŒ–
    schedule_phase0 = optax.exponential_decay(
        init_value=3e-3,
        transition_steps=50,
        decay_rate=0.92
    )
    
    optimizer_phase0 = optax.chain(
        optax.clip_by_global_norm(1.0),
        optax.adam(learning_rate=schedule_phase0)
    )
    
    opt_state_phase0 = optimizer_phase0.init(params_phase0)
    grad_fn_phase0 = jax.grad(loss_phase0)
    
    for step in range(300):
        grads = grad_fn_phase0(params_phase0)
        updates, opt_state_phase0 = optimizer_phase0.update(grads, opt_state_phase0, params_phase0)
        params_phase0 = optax.apply_updates(params_phase0, updates)
        
        if step % 100 == 0 and verbose:
            loss = loss_phase0(params_phase0)
            print(f"  Step {step:3d}: Physics Loss = {loss:.6f}")
    
    if verbose:
        final_loss_phase0 = loss_phase0(params_phase0)
        print(f"\n  Phase 0å®Œäº†: Physics Loss = {final_loss_phase0:.6f}")
        print("  ç‰©ç†çš„ã«å¦¥å½“ãªFLCé¢ã®åˆæœŸåŒ–å®Œäº†")
    
    # ===========================
    # Phase 1: FLCå½¢çŠ¶ç¢ºç«‹ï¼‹æ®µéšçš„ãƒã‚¤ãƒŠãƒªçµ±åˆ
    # ===========================
    if verbose:
        print("\n" + "="*60)
        print(" Phase 1: FLCå½¢çŠ¶ç¢ºç«‹ï¼‹æ®µéšçš„ãƒã‚¤ãƒŠãƒªçµ±åˆ")
        print("="*60)
        print("  FLCã‚’åŸºç¤ã¨ã—ã¦æ®µéšçš„ã«ãƒã‚¤ãƒŠãƒªæ€§èƒ½ã‚’ç©ã¿ä¸Šã’")
    
    # Phase 0ã®çµæœã‚’ä½¿ç”¨
    params_main = params_phase0.copy()
    
    # FLCãƒ‡ãƒ¼ã‚¿æº–å‚™
    if flc_pts:
        # ä»®æƒ³FLCç‚¹ã‚’è¿½åŠ ï¼ˆãƒ‡ãƒ¼ã‚¿å¢—å¼·ï¼‰
        virtual_flc_pts = [
            FLCPoint(-0.3, 0.32, -0.096, 0.6, 1.0, "virtual_1"),
            FLCPoint(0.3, 0.25, 0.075, 0.6, 1.0, "virtual_2"),
            FLCPoint(-0.7, 0.38, -0.266, 0.6, 1.0, "virtual_3"),
            FLCPoint(0.7, 0.18, 0.126, 0.6, 1.0, "virtual_4"),
        ]
        
        all_flc_pts = flc_pts + virtual_flc_pts
        flc_pts_data = {
            'path_ratios': jnp.array([p.path_ratio for p in all_flc_pts]),
            'major_limits': jnp.array([p.major_limit for p in all_flc_pts]),
            'minor_limits': jnp.array([p.minor_limit for p in all_flc_pts])
        }
    
    # Step 1.1: Î²ç³»ç¢ºç«‹ï¼ˆFLC 100%ï¼‰
    if verbose:
        print("\n--- Step 1.1: Î²ç³»ç¢ºç«‹ï¼ˆFLCå½¢çŠ¶ã®åŸºç¤ï¼‰ ---")
        print("  æœ€é©åŒ–: beta_A, beta_bw, beta_A_pos")
        print("  æå¤±: FLC 100%")
    
    beta_keys = ['logit_beta_A', 'logit_beta_bw', 'logit_beta_A_pos']
    
    def loss_step1(params):
        # Î²ç³»ã®ã¿æœ€é©åŒ–ã€ä»–ã¯å›ºå®š
        active_params = {k: params[k] for k in beta_keys}
        params_temp = params_main.copy()
        params_temp.update(active_params)
        
        # FLCæå¤±ã®ã¿
        return loss_flc_true_jax(params_temp, flc_pts_data, mat_dict)
    
    # AdamWã§æœ€é©åŒ–
    beta_params = {k: params_main[k] for k in beta_keys}
    optimizer = optax.adamw(learning_rate=1e-2, weight_decay=1e-4)
    opt_state = optimizer.init(beta_params)
    grad_fn = jax.grad(loss_step1)
    
    for step in range(300):
        grads = grad_fn(beta_params)
        updates, opt_state = optimizer.update(grads, opt_state, beta_params)
        beta_params = optax.apply_updates(beta_params, updates)
        
        if step % 100 == 0 and verbose:
            loss = loss_step1(beta_params)
            print(f"    Step {step}: FLC Loss = {loss:.6f}")
    
    params_main.update(beta_params)
    
    # Step 1.2: K_scaleç³»è¿½åŠ ï¼ˆFLC 90% + Binary 10%ï¼‰
    if verbose:
        print("\n--- Step 1.2: K_scaleç³»è¿½åŠ  ---")
        print("  æœ€é©åŒ–: K_scale_draw/plane/biax, triax_sens")
        print("  æå¤±: FLC 90% + Binary 10%")
    
    k_keys = ['logit_K_scale', 'logit_K_scale_draw', 
              'logit_K_scale_plane', 'logit_K_scale_biax', 'logit_triax_sens']
    
    def loss_step2(params):
        # Kç³»ã®ã¿æœ€é©åŒ–ã€Î²ç³»ã¯å›ºå®š
        active_params = {k: params[k] for k in k_keys}
        params_temp = params_main.copy()
        params_temp.update(active_params)
        
        # FLC 90% + Binary 10%
        flc_loss = loss_flc_true_jax(params_temp, flc_pts_data, mat_dict)
        bin_loss = loss_fn_jax(params_temp, exps, mat)
        return 0.9 * flc_loss + 0.1 * bin_loss
    
    k_params = {k: params_main[k] for k in k_keys}
    optimizer = optax.adamw(learning_rate=8e-3, weight_decay=1e-3)
    opt_state = optimizer.init(k_params)
    grad_fn = jax.grad(loss_step2)
    
    for step in range(300):
        grads = grad_fn(k_params)
        updates, opt_state = optimizer.update(grads, opt_state, k_params)
        k_params = optax.apply_updates(k_params, updates)
        
        if step % 100 == 0 and verbose:
            loss = loss_step2(k_params)
            print(f"    Step {step}: Mixed Loss = {loss:.6f}")
    
    params_main.update(k_params)
    
    # Step 1.3: Vç³»è¿½åŠ ï¼ˆFLC 70% + Binary 30%ï¼‰
    if verbose:
        print("\n--- Step 1.3: Vç³»è¿½åŠ  ---")
        print("  æœ€é©åŒ–: V0, av, ad, chi")
        print("  æå¤±: FLC 70% + Binary 30%")
    
    v_keys = ['log_V0', 'log_av', 'log_ad', 'logit_chi']
    
    def loss_step3(params):
        # Vç³»ã®ã¿æœ€é©åŒ–ã€Î²ç³»ãƒ»Kç³»ã¯å›ºå®š
        active_params = {k: params[k] for k in v_keys}
        params_temp = params_main.copy()
        params_temp.update(active_params)
        
        # FLC 70% + Binary 30%
        flc_loss = loss_flc_true_jax(params_temp, flc_pts_data, mat_dict)
        bin_loss = loss_fn_jax(params_temp, exps, mat)
        return 0.7 * flc_loss + 0.3 * bin_loss
    
    # L-BFGS-Bã§ç²¾å¯†æœ€é©åŒ–
    def loss_v_numpy(v_array):
        v_dict = {v_keys[i]: jnp.array(v_array[i]) for i in range(len(v_keys))}
        return float(loss_step3(v_dict))
    
    v_init = np.array([float(params_main[k]) for k in v_keys])
    res_v = minimize(loss_v_numpy, v_init, method='L-BFGS-B',
                    options={'maxiter': 50, 'ftol': 1e-8})
    
    if verbose:
        print(f"    L-BFGS-B: Mixed Loss = {res_v.fun:.6f}, iterations = {res_v.nit}")
    
    for i, key in enumerate(v_keys):
        params_main[key] = jnp.array(res_v.x[i])
    
    # Step 1.4: Lambda_critèª¿æ•´ï¼ˆFLC 50% + Binary 50%ï¼‰
    if verbose:
        print("\n--- Step 1.4: Lambda_critèª¿æ•´ ---")
        print("  æœ€é©åŒ–: Lambda_crit")
        print("  æå¤±: FLC 50% + Binary 50%")
    
    def loss_step4(lam):
        params_temp = params_main.copy()
        params_temp['Lambda_crit'] = lam
        
        # FLC 50% + Binary 50%
        flc_loss = loss_flc_true_jax(params_temp, flc_pts_data, mat_dict)
        bin_loss = loss_fn_jax(params_temp, exps, mat)
        return 0.5 * flc_loss + 0.5 * bin_loss
    
    # å‹¾é…é™ä¸‹ã§å¾®èª¿æ•´
    lam = params_main['Lambda_crit']
    learning_rate = 2e-3
    
    for step in range(50):
        grad = jax.grad(loss_step4)(lam)
        lam = lam - learning_rate * grad
        lam = jnp.clip(lam, 0.9, 1.1)
        
        if step % 25 == 0 and verbose:
            loss = loss_step4(lam)
            print(f"    Step {step}: Mixed Loss = {loss:.6f}, Lambda_crit = {float(lam):.4f}")
    
    params_main['Lambda_crit'] = lam
    
    # Step 1.5: å…¨ä½“å¾®èª¿æ•´ï¼ˆåˆ¶ç´„ä»˜ãBinaryæœ€é©åŒ–ï¼‰
    if verbose:
        print("\n--- Step 1.5: FLC Shaping + åˆ¶ç´„ä»˜ãBinaryæœ€é©åŒ– ---")
        print("  Step 0-1000: FLCæœ€é©åŒ–ï¼ˆç›®æ¨™: < 0.025ï¼‰")
        print("  Step 1000-1500: Binaryæœ€é©åŒ–ï¼ˆåˆ¶ç´„: FLC < 0.026, 3%è¨±å®¹ï¼‰")
    
    # åˆ¶ç´„ä»˜ãæå¤±é–¢æ•°ï¼ˆå‹•çš„é‡ã¿ä»˜ã‘ç‰ˆï¼‰
    def loss_constrained(params, flc_pts_data, exps, mat_dict, flc_target):
        flc_loss = loss_flc_true_jax(params, flc_pts_data, mat_dict)
        bin_loss = loss_fn_jax(params, exps, mat)
        
        # FLCé–¾å€¤ï¼ˆ3%è¨±å®¹ï¼‰
        flc_threshold = flc_target * 1.03
        
        # FLCãŒé–¾å€¤ã‹ã‚‰ã©ã‚Œã ã‘é›¢ã‚Œã¦ã„ã‚‹ã‹
        flc_margin = (flc_loss - flc_threshold) / (flc_threshold * 0.01)
        
        # sigmoidçš„ã«é‡ã¿ã‚’åˆ‡ã‚Šæ›¿ãˆ
        # flc_margin < 0ï¼ˆé–¾å€¤å†…ï¼‰â†’ w_flc â‰ˆ 0.1ï¼ˆBinaryé‡è¦–ï¼‰
        # flc_margin > 0ï¼ˆé–¾å€¤è¶…ï¼‰â†’ w_flc â‰ˆ 0.9ï¼ˆFLCé‡è¦–ï¼‰
        w_transition = jax.nn.sigmoid(flc_margin * 5.0)
        w_flc = 0.1 + 0.8 * w_transition
        w_bin = 1.0 - w_flc
        
        return w_flc * flc_loss + w_bin * bin_loss, flc_loss, bin_loss, w_flc, w_bin
    
    # warmupä»˜ãcosineã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
    schedule_flc = optax.warmup_cosine_decay_schedule(
        init_value=2e-3,
        peak_value=5e-3,
        warmup_steps=200,
        decay_steps=1300,
        end_value=5e-4
    )
    
    optimizer_final = optax.chain(
        optax.clip_by_global_norm(0.3),
        optax.adamw(learning_rate=schedule_flc, weight_decay=5e-5)
    )
    opt_state_final = optimizer_final.init(params_main)
    grad_fn_flc = jax.grad(loss_flc_true_jax)
    
    # FLCç›®æ¨™å€¤ï¼ˆPhase 1.5Aå¾Œã«è¨­å®šï¼‰
    flc_target_value = None
    
    for step in range(1500):
        # Step 0ï½1000: FLCæœ€é©åŒ–
        if step < 1000:
            grads = grad_fn_flc(params_main, flc_pts_data, mat_dict)
        # Step 1000ï½1500: åˆ¶ç´„ä»˜ãBinaryæœ€é©åŒ–
        else:
            # Step 1000ã§FLCç›®æ¨™å€¤ã‚’ç¢ºå®š
            if flc_target_value is None:
                flc_target_value = float(loss_flc_true_jax(params_main, flc_pts_data, mat_dict))
                if verbose:
                    print(f"\n    >>> FLCç›®æ¨™å€¤ç¢ºå®š: {flc_target_value:.6f}")
                    print(f"    >>> FLCé–¾å€¤ï¼ˆ3%è¨±å®¹ï¼‰: {flc_target_value * 1.03:.6f}")
                    print(f"    >>> åˆ¶ç´„ä»˜ãBinaryæœ€é©åŒ–é–‹å§‹ï¼\n")
            
            # åˆ¶ç´„ä»˜ãæå¤±é–¢æ•°ã§å‹¾é…è¨ˆç®—
            grad_fn_constrained = jax.grad(
                lambda p: loss_constrained(p, flc_pts_data, exps, mat_dict, flc_target_value)[0]
            )
            grads = grad_fn_constrained(params_main)
        
        updates, opt_state_final = optimizer_final.update(grads, opt_state_final, params_main)
        params_main = optax.apply_updates(params_main, updates)
        
        if step % 300 == 0 and verbose:
            flc_loss = loss_flc_true_jax(params_main, flc_pts_data, mat_dict)
            bin_loss = loss_fn_jax(params_main, exps, mat)
            
            if step >= 1000:
                # åˆ¶ç´„ä»˜ãæå¤±ã‚’è¨ˆç®—
                total_loss, _, _, w_flc, w_bin = loss_constrained(
                    params_main, flc_pts_data, exps, mat_dict, flc_target_value
                )
                flc_violation = max(0.0, float(flc_loss - flc_target_value * 1.03))
                
                print(f"    Step {step:4d}: Total = {float(total_loss):.6f} "
                      f"(FLC: {flc_loss:.6f}, Binary: {bin_loss:.6f})")
                print(f"              é‡ã¿(FLC: {float(w_flc):.2f}, Binary: {float(w_bin):.2f}), "
                      f"åˆ¶ç´„é•å: {flc_violation:.6f}")
            else:
                print(f"    Step {step:4d}: FLC Loss = {flc_loss:.6f} "
                      f"(Binaryå‚è€ƒ: {bin_loss:.6f})")
    
    # æœ€çµ‚çµæœ
    edr_dict_final = transform_params_jax(params_main)
    edr_final = edr_dict_to_dataclass(edr_dict_final)
    
    if verbose:
        # æœ€çµ‚æå¤±ã‚’è¨ˆç®—
        final_flc_loss = loss_flc_true_jax(params_main, flc_pts_data, mat_dict)
        final_bin_loss = loss_fn_jax(params_main, exps, mat)
        final_loss = 0.9 * final_flc_loss + 0.1 * final_bin_loss
        
        print(f"\n  Phase 1å®Œäº†: æœ€çµ‚Loss = {final_loss:.6f}")
        print(f"    FLC Loss = {final_flc_loss:.6f}")
        print(f"    Binary Loss = {final_bin_loss:.6f}")
    
    # ===========================
    # æœ€çµ‚æ¤œè¨¼
    # ===========================
    if verbose:
        print("\n" + "="*60)
        print(" æœ€é©åŒ–å®Œäº†ï¼")
        print("="*60)
        
        # Final Validation
        print("\n=== Final Validation ===")
        correct = 0
        
        edr_dict_for_validation = transform_params_jax(params_main)
        
        for i, exp in enumerate(exps):
            schedule_dict = schedule_to_jax_dict(exp.schedule)
            res = simulate_lambda_jax(schedule_dict, mat_dict, edr_dict_for_validation)
            Lambda_smooth = smooth_signal_jax(res["Lambda"], window_size=11)
            peak = float(jnp.max(Lambda_smooth))
            D_end = float(res["Damage"][-1])
            
            Dcrit = 0.01
            if exp.failed == 1:
                passed = (peak > edr_final.Lambda_crit and D_end > Dcrit)
            else:
                passed = (peak < edr_final.Lambda_crit - 0.03)
            
            if passed:
                correct += 1
                status = "âœ“"
            else:
                status = "âœ—"
                
            print(f"Exp{i}({exp.label}): Î›_max={peak:.3f}, D={D_end:.4f}, "
                  f"failed={exp.failed}, {status}")
        
        accuracy = correct / len(exps) * 100
        print(f"Accuracy: {accuracy:.2f}%")
        
        final_binary_loss = loss_fn_jax(params_main, exps, mat)
        print(f"Final binary loss: {final_binary_loss:.4f}")
    
    info = {
        'success': True,
        'final_loss': float(final_loss),
        'final_flc_loss': float(final_flc_loss),
        'final_bin_loss': float(final_bin_loss),
        'phase0_loss': float(final_loss_phase0),
        'phase1_loss': float(final_loss),
    }
    
    return edr_final, info

# =============================================================================
# Section 6: ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# =============================================================================

def predict_FLC_point(path_ratio: float, major_rate: float, duration_max: float,
                     mat: MaterialParams, edr: EDRParams,
                     base_contact: float=1.0, base_mu: float=0.08,
                     base_pN: float=200e6, base_vslip: float=0.02,
                     base_htc: float=8000.0, Tdie: float=293.15,
                     T0: float=293.15) -> Tuple[float, float]:
    """FLCç‚¹äºˆæ¸¬"""
    dt = 1e-3
    N = int(duration_max/dt) + 1
    t = np.linspace(0, duration_max, N)
    epsM = major_rate * t
    epsm = path_ratio * major_rate * t
    
    schedule = PressSchedule(
        t=t, eps_maj=epsM, eps_min=epsm,
        triax=np.full(N, float(triax_from_path_jax(path_ratio))),
        mu=np.full(N, base_mu), pN=np.full(N, base_pN),
        vslip=np.full(N, base_vslip), htc=np.full(N, base_htc),
        Tdie=np.full(N, Tdie), contact=np.full(N, base_contact), T0=T0
    )
    
    # JAXç‰ˆã§å®Ÿè¡Œ
    schedule_dict = schedule_to_jax_dict(schedule)
    mat_dict = mat_to_jax_dict(mat)
    edr_dict = {
        'V0': edr.V0, 'av': edr.av, 'ad': edr.ad, 'chi': edr.chi,
        'K_scale': edr.K_scale, 'triax_sens': edr.triax_sens,
        'Lambda_crit': edr.Lambda_crit,
        'K_scale_draw': edr.K_scale_draw,
        'K_scale_plane': edr.K_scale_plane,
        'K_scale_biax': edr.K_scale_biax,
        'beta_A': edr.beta_A, 'beta_bw': edr.beta_bw,
        'beta_A_pos': edr.beta_A_pos
    }
    
    res = simulate_lambda_jax(schedule_dict, mat_dict, edr_dict)
    Lambda = np.array(res["Lambda"])
    
    # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°å‡¦ç†
    Lambda_smooth = np.array(smooth_signal_jax(jnp.array(Lambda), window_size=11))
    
    # é™ç•Œç‚¹ã‚’æ¢ã™
    idx = np.where(Lambda_smooth > edr.Lambda_crit)[0]
    if len(idx) > 0:
        k = idx[0]
        return float(epsM[k]), float(epsm[k])
    else:
        return float(epsM[-1]), float(epsm[-1])

def evaluate_flc_fit(experimental: List[FLCPoint],
                    predicted: List[Tuple[float, float]]) -> float:
    """FLCé©åˆåº¦è©•ä¾¡"""
    errors = []
    for exp, pred in zip(experimental, predicted):
        deM = pred[0] - exp.major_limit
        dem = pred[1] - exp.minor_limit
        err = np.sqrt(deM**2 + dem**2)
        errors.append(err)
        print(f"  Î²={exp.path_ratio:+.1f}: èª¤å·®={err:.4f} (Î”Maj={deM:+.3f}, Î”Min={dem:+.3f})")
    
    mean_err = np.mean(errors)
    max_err = np.max(errors)
    
    print(f"\nFLCé©åˆåº¦è©•ä¾¡:")
    print(f"  å¹³å‡èª¤å·®: {mean_err:.4f}")
    print(f"  æœ€å¤§èª¤å·®: {max_err:.4f}")
    print(f"  ç²¾åº¦è©•ä¾¡: ", end="")
    
    if mean_err < 0.05:
        print("âœ… å„ªç§€ï¼ˆ<5%ï¼‰")
    elif mean_err < 0.10:
        print("ğŸŸ¡ è‰¯å¥½ï¼ˆ<10%ï¼‰")
    elif mean_err < 0.20:
        print("ğŸŸ  è¦æ”¹å–„ï¼ˆ<20%ï¼‰")
    else:
        print("ğŸ”´ ä¸è‰¯ï¼ˆ>20%ï¼‰")
    
    return mean_err

# =============================================================================
# Section 7: ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
# =============================================================================

def generate_demo_experiments() -> List[ExpBinary]:
    """ãƒ‡ãƒ¢å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    def mk_schedule(beta, mu_base, mu_jump=False, high_stress=False):
        dt = 1e-3
        T = 0.6
        t = np.arange(0, T+dt, dt)
        
        if high_stress:
            epsM = 0.5 * (t/T)**0.8
        else:
            epsM = 0.35 * (t/T)
        epsm = beta * epsM
        
        mu = np.full_like(t, mu_base)
        if mu_jump:
            j = int(0.25/dt)
            mu[j:] += 0.06
        
        triax_val = float(triax_from_path_jax(beta))
        
        return PressSchedule(
            t=t, eps_maj=epsM, eps_min=epsm,
            triax=np.full_like(t, triax_val), mu=mu,
            pN=np.full_like(t, 250e6 if high_stress else 200e6),
            vslip=np.full_like(t, 0.03), htc=np.full_like(t, 8000.0),
            Tdie=np.full_like(t, 293.15), contact=np.full_like(t, 1.0), T0=293.15
        )
    
    exps = [
        ExpBinary(mk_schedule(-0.5, 0.08, False, False), failed=0, label="safe_draw"),
        ExpBinary(mk_schedule(-0.5, 0.08, True, True), failed=1, label="draw_fail"),
        ExpBinary(mk_schedule(0.0, 0.08, False, False), failed=0, label="safe_plane"),
        ExpBinary(mk_schedule(0.0, 0.08, True, True), failed=1, label="plane_fail"),
        ExpBinary(mk_schedule(0.5, 0.10, False, False), failed=0, label="safe_biax"),
        ExpBinary(mk_schedule(0.5, 0.10, True, True), failed=1, label="biax_fail"),
    ]
    return exps

def generate_demo_flc() -> List[FLCPoint]:
    """ãƒ‡ãƒ¢FLCãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    return [
        FLCPoint(-0.5, 0.35, -0.175, 0.6, 1.0, "draw"),
        FLCPoint(0.0, 0.28, 0.0, 0.6, 1.0, "plane"),
        FLCPoint(0.5, 0.22, 0.11, 0.6, 1.0, "biax"),
    ]

# =============================================================================
# Section 8: ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# =============================================================================

if __name__ == "__main__":
    print("="*80)
    print(" EDRãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµ±åˆç‰ˆ v5.2")
    print(" + Operation Marie Antoinetteï¼ˆé€†å•é¡Œãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼‰")
    print(" Inverse-EDR Neural Calibration Engine (IENCE)")
    print("="*80)
    
    # ææ–™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    mat = MaterialParams()
    
    # ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    print("\n[ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ]")
    exps = generate_demo_experiments()
    flc_data = generate_demo_flc()
    print(f"  å®Ÿé¨“æ•°: {len(exps)}")
    print(f"  FLCç‚¹æ•°: {len(flc_data)}")
    
    # å¤šæ®µéšHybridæœ€é©åŒ–å®Ÿè¡Œ
    edr_fit, info = hybrid_staged_optimization(
        exps, flc_data, mat,
        verbose=True
    )
    
    # çµæœè¡¨ç¤º
    print("\n" + "="*60)
    print(" æœ€çµ‚çµæœ")
    print("="*60)
    print(f"\næœ€çµ‚Loss: {info['final_loss']:.6f}")
    print(f"  FLC Loss: {info['final_flc_loss']:.6f}")
    print(f"  Binary Loss: {info['final_bin_loss']:.6f}")
    print(f"Phase0 Loss: {info['phase0_loss']:.6f}")
    print(f"Phase1 Loss: {info['phase1_loss']:.6f}")
    
    print(f"\nEDR Parameters:")
    print(f"  V0: {edr_fit.V0:.2e} Pa")
    print(f"  av: {edr_fit.av:.2e}")
    print(f"  ad: {edr_fit.ad:.2e}")
    print(f"  chi: {edr_fit.chi:.3f}")
    print(f"  K_scale: {edr_fit.K_scale:.3f}")
    print(f"  triax_sens: {edr_fit.triax_sens:.3f}")
    print(f"  Lambda_crit: {edr_fit.Lambda_crit:.3f}")
    print(f"  K_scale_draw: {edr_fit.K_scale_draw:.3f}")
    print(f"  K_scale_plane: {edr_fit.K_scale_plane:.3f}")
    print(f"  K_scale_biax: {edr_fit.K_scale_biax:.3f}")
    print(f"  beta_A: {edr_fit.beta_A:.3f}")
    print(f"  beta_bw: {edr_fit.beta_bw:.3f}")
    print(f"  beta_A_pos: {edr_fit.beta_A_pos:.3f} (éå¯¾ç§°)")
    
    # FLCäºˆæ¸¬
    print("\n[FLCäºˆæ¸¬]")
    preds = []
    for p in flc_data:
        Em, em = predict_FLC_point(p.path_ratio, p.rate_major, p.duration_max, mat, edr_fit)
        preds.append((Em, em))
        print(f"  Î²={p.path_ratio:+.1f}: å®Ÿæ¸¬({p.major_limit:.3f}, {p.minor_limit:.3f}) "
              f"â†’ äºˆæ¸¬({Em:.3f}, {em:.3f})")
    
    flc_error = evaluate_flc_fit(flc_data, preds)
    
    # ğŸ‚ Operation Marie Antoinetteï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if MARIE_ANTOINETTE_AVAILABLE:
        print("\n" + "="*80)
        print(" ğŸ‚ Operation Marie Antoinette: å®‰å…¨å¤šæ§˜ä½“åˆ†æ")
        print("="*80)
        
        use_marie = input("\nå®‰å…¨å¤šæ§˜ä½“ã‚’æ§‹ç¯‰ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").lower().strip()
        
        if use_marie == 'y':
            # ææ–™ãƒ»EDRãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’JAXå½¢å¼ã«
            mat_dict = mat_to_jax_dict(mat)
            edr_dict = {
                'V0': edr_fit.V0, 'av': edr_fit.av, 'ad': edr_fit.ad,
                'chi': edr_fit.chi, 'K_scale': edr_fit.K_scale,
                'triax_sens': edr_fit.triax_sens,
                'Lambda_crit': edr_fit.Lambda_crit,
                'K_scale_draw': edr_fit.K_scale_draw,
                'K_scale_plane': edr_fit.K_scale_plane,
                'K_scale_biax': edr_fit.K_scale_biax,
                'beta_A': edr_fit.beta_A, 'beta_bw': edr_fit.beta_bw,
                'beta_A_pos': edr_fit.beta_A_pos
            }
            
            # å®‰å…¨å¤šæ§˜ä½“æ§‹ç¯‰
            safe_manifold = build_safe_manifold(
                mat_dict, edr_dict, simulate_lambda_jax,
                n_beta=15, n_mu=5, n_pN=5,
                verbose=True
            )
            
            # å¯è¦–åŒ–
            visualize_safe_manifold(safe_manifold, 
                                   output_path='/home/claude/safe_manifold.png')
            
            # å®‰å…¨ã‚¹ã‚³ã‚¢åˆ†æ
            manifold_weights = {
                'tv': 0.1,
                'jump': 0.5,
                'topo': 0.1,
                'l1': 1e-3
            }
            
            analyze_safety_scores(
                exps, mat_dict, edr_dict, safe_manifold,
                simulate_lambda_jax, manifold_weights
            )
            
            print("\nâœ… Operation Marie Antoinetteå®Œäº†ï¼")
            print("  å®‰å…¨å¤šæ§˜ä½“ã‚’ä½¿ã£ãŸãƒã‚¤ãƒŠãƒªåˆ¤å®šãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸ")
    
    print("\n" + "="*80)
    print(" å®Ÿè¡Œå®Œäº†ï¼")
    print(" éå¯¾ç§°FLCå¯¾å¿œãƒ»å¤šæ®µéšæœ€é©åŒ–å®Œæˆ âœ…")
    print(" ğŸ‚ Operation Marie Antoinetteçµ±åˆç‰ˆ")
    print("="*80)
